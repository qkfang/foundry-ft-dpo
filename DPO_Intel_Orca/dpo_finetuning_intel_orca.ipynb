{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Microsoft Foundry\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Microsoft Foundry\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model\n",
        "\n",
        "Note: Execute each cell in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Evaluation Function\n",
        "\n",
        "Function to evaluate model performance using Azure AI Evaluation SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(deployment_name, num_samples=10, evaluator_model=None):\n",
        "    \"\"\"\n",
        "    Evaluate a model deployment using Azure AI Evaluation SDK.\n",
        "    \n",
        "    Args:\n",
        "        deployment_name: Name of the deployed model to evaluate\n",
        "        num_samples: Number of samples to evaluate (default: 10)\n",
        "        evaluator_model: Name of the model to use for evaluation (default: use base model from env)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from azure.ai.evaluation import evaluate, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
        "    from openai import AzureOpenAI\n",
        "    \n",
        "    if evaluator_model is None:\n",
        "        evaluator_model = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    \n",
        "    print(f\"Evaluating deployment: {deployment_name}\")\n",
        "    print(f\"Using evaluator model: {evaluator_model}\")\n",
        "    print(f\"Using {num_samples} samples from training.jsonl\")\n",
        "    \n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        api_version=\"2024-08-01-preview\"\n",
        "    )\n",
        "    \n",
        "    print(\"Generating model responses...\")\n",
        "    eval_data = []\n",
        "    with open(\"training.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            sample = json.loads(line)\n",
        "            \n",
        "            messages = sample[\"input\"][\"messages\"]\n",
        "            query = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "            \n",
        "            response = azure_openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            model_response = response.choices[0].message.content\n",
        "            \n",
        "            ground_truth = next((msg[\"content\"] for msg in sample[\"preferred_output\"] if msg[\"role\"] == \"assistant\"), \"\")\n",
        "            \n",
        "            eval_data.append({\n",
        "                \"query\": query,\n",
        "                \"response\": model_response,\n",
        "                \"ground_truth\": ground_truth\n",
        "            })\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == num_samples:\n",
        "                print(f\"  Processed {i+1}/{num_samples}\")\n",
        "    \n",
        "    eval_file = f\"evaluation_data_{deployment_name.replace('-', '_')}.jsonl\"\n",
        "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "        for item in eval_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    model_config = {\n",
        "        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        \"azure_deployment\": evaluator_model,\n",
        "        \"api_version\": \"2024-08-01-preview\",\n",
        "    }\n",
        "    \n",
        "    print(\"Running evaluation with 3 metrics...\")\n",
        "    try:\n",
        "        results = evaluate(\n",
        "            data=eval_file,\n",
        "            evaluators={\n",
        "                \"coherence\": CoherenceEvaluator(model_config=model_config),\n",
        "                \"fluency\": FluencyEvaluator(model_config=model_config),\n",
        "                \"groundedness\": GroundednessEvaluator(model_config=model_config)\n",
        "            },\n",
        "            evaluator_config={\n",
        "                \"default\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"ground_truth\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                },\n",
        "                \"groundedness\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"context\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            output_path=f\"./evaluation_results_{deployment_name.replace('-', '_')}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation encountered an error: {str(e)}\")\n",
        "        print(\"Results may be incomplete. Check the output folder for partial results.\")\n",
        "        results = {\"metrics\": {}, \"error\": str(e)}\n",
        "    \n",
        "    print(f\"EVALUATION RESULTS: {deployment_name}\\n\")\n",
        "    \n",
        "    if \"metrics\" in results:\n",
        "        metrics = results[\"metrics\"]\n",
        "        \n",
        "        coherence = metrics.get('coherence.coherence', metrics.get('coherence'))\n",
        "        fluency = metrics.get('fluency.fluency', metrics.get('fluency'))\n",
        "        groundedness = metrics.get('groundedness.groundedness', metrics.get('groundedness'))\n",
        "        \n",
        "        if coherence is not None:\n",
        "            print(f\"Coherence:      {coherence:.4f} (1-5 scale)\")\n",
        "        if fluency is not None:\n",
        "            print(f\"Fluency:        {fluency:.4f} (1-5 scale)\")\n",
        "        if groundedness is not None:\n",
        "            print(f\"Groundedness:   {groundedness:.4f} (1-5 scale)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"Detailed results: ./evaluation_results_{deployment_name.replace('-', '_')}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure Azure Environment\n",
        "Set your Microsoft Foundry Project endpoint and model name. We're using **gpt-4.1-mini** in this example, but you can use other supported GPT models. Copy the file `.env.template` (located in this folder), and save it as file named `.env`. Enter appropriate values for the environment variables used for the job you want to run. \n",
        "\n",
        "```\n",
        "# Required for DPO Fine-Tuning\n",
        "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint> \n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "MODEL_NAME=<your-base-model-name>\n",
        "\n",
        "# Required for Model Local Evaluation\n",
        "AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>\n",
        "AZURE_OPENAI_KEY=<your-azure-openai-api-key>\n",
        "DEPLOYMENT_NAME=<your-deployment-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\"Base model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Microsoft Foundry Project\n",
        "\n",
        "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\"Connected to Microsoft Foundry Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Base Model\n",
        "\n",
        "Establish baseline performance metrics by evaluating the base model before DPO fine-tuning. This provides a comparison point to measure improvements after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "print(f\"Evaluating base model: {base_deployment}\\n\")\n",
        "\n",
        "base_results = evaluate_model(base_deployment, num_samples=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 1,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4.1-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=200)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    print(\"Waiting for deployment to complete...\")\n",
        "    deployment.result()\n",
        "\n",
        "print(f\"Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate Fine-Tuned Model\n",
        "\n",
        "Evaluate your model using Azure AI Evaluation SDK to measure quality improvements from DPO fine-tuning.\n",
        "\n",
        "We'll assess 3 key metrics:\n",
        "- **Coherence**: Logical flow and structure\n",
        "- **Fluency**: Grammatical correctness and naturalness\n",
        "- **Groundedness**: Factual accuracy against context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = os.getenv(\"DEPLOYMENT_NAME\")  # Use base model for evaluation\n",
        "\n",
        "print(f\"Evaluating fine-tuned model: {deployment_name}\")\n",
        "\n",
        "finetuned_results = evaluate_model(deployment_name, num_samples=50, evaluator_model=base_model)\n",
        "\n",
        "print(\"\\nCompare base model vs fine-tuned model metrics to see DPO improvements!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.1 Model Comparison Results\n",
        "\n",
        "Below is an example comparison between base model and fine-tuned model evaluation results (using 50 samples):\n",
        "\n",
        "| Metric | Base Model | Fine-Tuned Model | Change | Status |\n",
        "|--------|-----------|------------------|--------|--------|\n",
        "| **Coherence** | 4.3000 | 3.8400 | -0.4600 | Decreased |\n",
        "| **Fluency** | 3.5800 | 2.7000 | -0.8800 | Decreased |\n",
        "| **Groundedness** | 4.1000 | 3.1000 | -1.0000 | Decreased |\n",
        "\n",
        "### Understanding the Results\n",
        "\n",
        "The example above shows that the fine-tuned model performed worse than the base model across all metrics. This indicates that the DPO training did not improve model quality with the current configuration.\n",
        "\n",
        "### How to Improve Results\n",
        "\n",
        "To achieve better fine-tuned model performance, experiment with:\n",
        "\n",
        "**1. Hyperparameter Tuning:**\n",
        "- **Reduce epochs**: Try `n_epochs=1` or `n_epochs=2` to prevent overfitting\n",
        "- **Lower learning rate**: Set `learning_rate_multiplier=0.5` or `0.1` for more conservative training\n",
        "- **Adjust batch size**: Keep at 1-2 for DPO stability\n",
        "\n",
        "**2. Training Data:**\n",
        "- **Increase sample size**: Use more training examples (e.g., 100-1000 samples)\n",
        "- **Verify data quality**: Ensure \"preferred_output\" responses are truly higher quality than rejected ones\n",
        "- **Review data format**: Confirm DPO pairs are correctly labeled\n",
        "\n",
        "**3. Evaluation Settings:**\n",
        "- **Increase evaluation samples**: Use `num_samples=100` or more for more reliable metrics\n",
        "- **Test on different data**: Evaluate on a separate validation set, not training data\n",
        "\n",
        "### Success Indicators\n",
        "\n",
        "Your fine-tuning is successful when you see **positive changes** like:\n",
        "- Coherence: +0.5 or higher\n",
        "- Fluency: +0.3 or higher  \n",
        "- Groundedness: +0.4 or higher\n",
        "\n",
        "Iterate on hyperparameters and training data until your fine-tuned model consistently outperforms the base model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Continual Fine-Tuning (Optional)\n",
        "\n",
        "If your fine-tuned model didn't show improvements, you can perform **continual fine-tuning** by using the fine-tuned model as the base for another round of training. This iterative approach can help refine the model further.\n",
        "\n",
        "### When to Use Continual Fine-Tuning:\n",
        "- Your first fine-tuning run didn't improve metrics\n",
        "- You want to adjust hyperparameters and train further\n",
        "- You have additional training data to incorporate\n",
        "- You need to fine-tune for a more specific task\n",
        "\n",
        "### How It Works:\n",
        "Instead of using model_name (base model), use fine_tuned_model_id from section 10 as your new base model. The code below is the same as section 8, but modified to continue training from your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "continual_base_model = fine_tuned_model_id\n",
        "print(f\"Continual fine-tuning using base model: {continual_base_model}\")\n",
        "\n",
        "continual_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,  # You can use the same or upload new training data\n",
        "    validation_file=validation_file.id,\n",
        "    model=continual_base_model,  # Using fine-tuned model instead of base model\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 2,  # Reduced from 3 to prevent overfitting\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 0.5  # Lower learning rate for fine-tuning refinement\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\"Continual fine-tuning job created!\")\n",
        "print(f\"Job ID: {continual_job.id}\")\n",
        "print(f\"Status: {continual_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
