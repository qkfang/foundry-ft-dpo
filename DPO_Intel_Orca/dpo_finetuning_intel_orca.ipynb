{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Microsoft Foundry\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Microsoft Foundry\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model\n",
        "\n",
        "Note: Execute each cell in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-ai-projects>=2.0.0b1 in c:\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (2.0.0b3)\n",
            "Requirement already satisfied: openai in c:\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (2.15.0)\n",
            "Requirement already satisfied: azure-identity in c:\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 8)) (1.25.1)\n",
            "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
            "  Downloading azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting azure-ai-evaluation>=1.13.0 (from -r requirements.txt (line 12))\n",
            "  Downloading azure_ai_evaluation-1.15.3-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: python-dotenv in c:\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
            "Requirement already satisfied: isodate>=0.6.1 in c:\\python\\python312\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.35.0 in c:\\python\\python312\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (1.38.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\python\\python312\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (12.28.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (2.12.5)\n",
            "Requirement already satisfied: sniffio in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in c:\\python\\python312\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (46.0.5)\n",
            "Requirement already satisfied: msal>=1.30.0 in c:\\python\\python312\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (1.34.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\python\\python312\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: msrest>=0.7.1 in c:\\python\\python312\\lib\\site-packages (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: azure-mgmt-core>=1.6.0 in c:\\python\\python312\\lib\\site-packages (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in c:\\python\\python312\\lib\\site-packages (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.10.1)\n",
            "Collecting nltk>=3.9.1 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Downloading nltk-3.9.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas<3.0.0,>=2.1.2 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: ruamel.yaml<1.0.0,>=0.17.10 in c:\\python\\python312\\lib\\site-packages (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (0.19.1)\n",
            "Collecting Jinja2>=3.1.6 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: aiohttp>=3.0 in c:\\python\\python312\\lib\\site-packages (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (3.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\python\\python312\\lib\\site-packages (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: requests>=2.21.0 in c:\\python\\python312\\lib\\site-packages (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\python\\python312\\lib\\site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: certifi in c:\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python312\\lib\\site-packages (from Jinja2>=3.1.6->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.1.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\python\\python312\\lib\\site-packages (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: click in c:\\python\\python312\\lib\\site-packages (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (8.1.7)\n",
            "Collecting joblib (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Downloading regex-2026.2.19-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\python\\python312\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python312\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python312\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\python312\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python312\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: pycparser in c:\\python\\python312\\lib\\site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8)) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\python312\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (3.3.1)\n",
            "Downloading azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
            "Downloading azure_ai_evaluation-1.15.3-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.2/1.2 MB 11.8 MB/s eta 0:00:00\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading nltk-3.9.3-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 10.1 MB/s eta 0:00:00\n",
            "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 2.4/11.0 MB 12.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 5.0/11.0 MB 12.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 7.6/11.0 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.0/11.0 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 11.6 MB/s eta 0:00:00\n",
            "Downloading regex-2026.2.19-cp312-cp312-win_amd64.whl (277 kB)\n",
            "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Installing collected packages: regex, joblib, Jinja2, pandas, nltk, azure-mgmt-cognitiveservices, azure-ai-evaluation\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 3.0.0\n",
            "    Uninstalling pandas-3.0.0:\n",
            "      Successfully uninstalled pandas-3.0.0\n",
            "Successfully installed Jinja2-3.1.6 azure-ai-evaluation-1.15.3 azure-mgmt-cognitiveservices-14.1.0 joblib-1.5.3 nltk-3.9.3 pandas-2.3.3 regex-2026.2.19\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "semantic-kernel 1.30.0 requires pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Evaluation Function\n",
        "\n",
        "Function to evaluate model performance using Azure AI Evaluation SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(deployment_name, num_samples=10, evaluator_model=None):\n",
        "    \"\"\"\n",
        "    Evaluate a model deployment using Azure AI Evaluation SDK.\n",
        "    \n",
        "    Args:\n",
        "        deployment_name: Name of the deployed model to evaluate\n",
        "        num_samples: Number of samples to evaluate (default: 10)\n",
        "        evaluator_model: Name of the model to use for evaluation (default: use base model from env)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from azure.ai.evaluation import evaluate, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
        "    from openai import AzureOpenAI\n",
        "    \n",
        "    if evaluator_model is None:\n",
        "        evaluator_model = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    \n",
        "    print(f\"Evaluating deployment: {deployment_name}\")\n",
        "    print(f\"Using evaluator model: {evaluator_model}\")\n",
        "    print(f\"Using {num_samples} samples from training.jsonl\")\n",
        "    \n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        api_version=\"2024-08-01-preview\"\n",
        "    )\n",
        "    \n",
        "    print(\"Generating model responses...\")\n",
        "    eval_data = []\n",
        "    with open(\"training.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            sample = json.loads(line)\n",
        "            \n",
        "            messages = sample[\"input\"][\"messages\"]\n",
        "            query = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "            \n",
        "            response = azure_openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            model_response = response.choices[0].message.content\n",
        "            \n",
        "            ground_truth = next((msg[\"content\"] for msg in sample[\"preferred_output\"] if msg[\"role\"] == \"assistant\"), \"\")\n",
        "            \n",
        "            eval_data.append({\n",
        "                \"query\": query,\n",
        "                \"response\": model_response,\n",
        "                \"ground_truth\": ground_truth\n",
        "            })\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == num_samples:\n",
        "                print(f\"  Processed {i+1}/{num_samples}\")\n",
        "    \n",
        "    eval_file = f\"evaluation_data_{deployment_name.replace('-', '_')}.jsonl\"\n",
        "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "        for item in eval_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    model_config = {\n",
        "        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        \"azure_deployment\": evaluator_model,\n",
        "        \"api_version\": \"2024-08-01-preview\",\n",
        "    }\n",
        "    \n",
        "    print(\"Running evaluation with 3 metrics...\")\n",
        "    try:\n",
        "        results = evaluate(\n",
        "            data=eval_file,\n",
        "            evaluators={\n",
        "                \"coherence\": CoherenceEvaluator(model_config=model_config),\n",
        "                \"fluency\": FluencyEvaluator(model_config=model_config),\n",
        "                \"groundedness\": GroundednessEvaluator(model_config=model_config)\n",
        "            },\n",
        "            evaluator_config={\n",
        "                \"default\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"ground_truth\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                },\n",
        "                \"groundedness\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"context\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            output_path=f\"./evaluation_results_{deployment_name.replace('-', '_')}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation encountered an error: {str(e)}\")\n",
        "        print(\"Results may be incomplete. Check the output folder for partial results.\")\n",
        "        results = {\"metrics\": {}, \"error\": str(e)}\n",
        "    \n",
        "    print(f\"EVALUATION RESULTS: {deployment_name}\\n\")\n",
        "    \n",
        "    if \"metrics\" in results:\n",
        "        metrics = results[\"metrics\"]\n",
        "        \n",
        "        coherence = metrics.get('coherence.coherence', metrics.get('coherence'))\n",
        "        fluency = metrics.get('fluency.fluency', metrics.get('fluency'))\n",
        "        groundedness = metrics.get('groundedness.groundedness', metrics.get('groundedness'))\n",
        "        \n",
        "        if coherence is not None:\n",
        "            print(f\"Coherence:      {coherence:.4f} (1-5 scale)\")\n",
        "        if fluency is not None:\n",
        "            print(f\"Fluency:        {fluency:.4f} (1-5 scale)\")\n",
        "        if groundedness is not None:\n",
        "            print(f\"Groundedness:   {groundedness:.4f} (1-5 scale)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"Detailed results: ./evaluation_results_{deployment_name.replace('-', '_')}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure Azure Environment\n",
        "Set your Microsoft Foundry Project endpoint and model name. We're using **gpt-4.1-mini** in this example, but you can use other supported GPT models. Copy the file `.env.template` (located in this folder), and save it as file named `.env`. Enter appropriate values for the environment variables used for the job you want to run. \n",
        "\n",
        "```\n",
        "# Required for DPO Fine-Tuning\n",
        "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint> \n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "MODEL_NAME=<your-base-model-name>\n",
        "\n",
        "# Required for Model Local Evaluation\n",
        "AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>\n",
        "AZURE_OPENAI_KEY=<your-azure-openai-api-key>\n",
        "DEPLOYMENT_NAME=<your-deployment-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\"Base model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Microsoft Foundry Project\n",
        "\n",
        "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Microsoft Foundry Project\n"
          ]
        }
      ],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\"Connected to Microsoft Foundry Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading training file...\n",
            " Training file ID: file-790562148477483a9ddccb51d4a02c7e\n",
            "\n",
            "Uploading validation file...\n",
            " Validation file ID: file-f1ef0a6914e046d6b35fa81ad6ca2f71\n"
          ]
        }
      ],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for files to be processed...\n",
            " Files ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Base Model\n",
        "\n",
        "Establish baseline performance metrics by evaluating the base model before DPO fine-tuning. This provides a comparison point to measure improvements after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "print(f\"Evaluating base model: {base_deployment}\\n\")\n",
        "\n",
        "base_results = evaluate_model(base_deployment, num_samples=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 1,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploying fine-tuned model: gpt-4.1-mini-2025-04-14.ft-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Waiting for deployment to complete...\n",
            "Model deployment completed: gpt-4.1-mini-dpo-finetuned\n"
          ]
        }
      ],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4.1-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=200)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    print(\"Waiting for deployment to complete...\")\n",
        "    deployment.result()\n",
        "\n",
        "print(f\"Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing fine-tuned model via deployment: gpt-4.1-mini-dpo-finetuned\n",
            "Model response: Machine learning is like teaching a computer to learn from experience, similar to how people do. Instead of programming specific instructions for every task, we give the computer a lot of data and it figures out patterns on its own. Then, it can use what it learned to make decisions or predictions. For example, if you show a machine learning system lots of pictures of cats and dogs, it will learn to recognize which is which by itself.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate Fine-Tuned Model\n",
        "\n",
        "Evaluate your model using Azure AI Evaluation SDK to measure quality improvements from DPO fine-tuning.\n",
        "\n",
        "We'll assess 3 key metrics:\n",
        "- **Coherence**: Logical flow and structure\n",
        "- **Fluency**: Grammatical correctness and naturalness\n",
        "- **Groundedness**: Factual accuracy against context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating fine-tuned model: gpt-4.1-mini-dpo-finetuned\n",
            "Using base model as evaluator: gpt-4.1-mini\n",
            "\n",
            "Evaluating deployment: gpt-4.1-mini-dpo-finetuned\n",
            "Using evaluator model: gpt-4.1-mini\n",
            "Using 50 samples from training.jsonl\n",
            "Generating model responses...\n",
            "  Processed 10/50\n",
            "  Processed 20/50\n",
            "  Processed 30/50\n",
            "  Processed 40/50\n",
            "  Processed 50/50\n",
            "Running evaluation with 3 metrics...\n",
            "2026-01-05 14:50:06 +0530   24016 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:06 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 8.14 seconds. Estimated time for incomplete lines: 398.86 seconds.\n",
            "2026-01-05 14:50:07 +0530   24016 execution.bulk     INFO     Finished 2 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 4.35 seconds. Estimated time for incomplete lines: 208.8 seconds.\n",
            "2026-01-05 14:50:07 +0530   22800 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 8.87 seconds. Estimated time for incomplete lines: 434.63 seconds.\n",
            "2026-01-05 14:50:07 +0530   28448 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 9.01 seconds. Estimated time for incomplete lines: 441.49 seconds.\n",
            "2026-01-05 14:50:09 +0530   28448 execution.bulk     INFO     Finished 4 / 50 lines.\n",
            "2026-01-05 14:50:09 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 2.73 seconds. Estimated time for incomplete lines: 125.58 seconds.\n",
            "2026-01-05 14:50:09 +0530   24016 execution.bulk     INFO     Finished 6 / 50 lines.\n",
            "2026-01-05 14:50:09 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.83 seconds. Estimated time for incomplete lines: 80.52 seconds.\n",
            "2026-01-05 14:50:10 +0530   22800 execution.bulk     INFO     Finished 7 / 50 lines.\n",
            "2026-01-05 14:50:10 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.76 seconds. Estimated time for incomplete lines: 75.68 seconds.\n",
            "2026-01-05 14:50:11 +0530   24016 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:11 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.32 seconds. Estimated time for incomplete lines: 52.8 seconds.\n",
            "2026-01-05 14:50:12 +0530   22800 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:12 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.39 seconds. Estimated time for incomplete lines: 55.6 seconds.\n",
            "2026-01-05 14:50:12 +0530   28448 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:12 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.44 seconds. Estimated time for incomplete lines: 57.6 seconds.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.43 seconds. Estimated time for incomplete lines: 55.77 seconds.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Finished 12 / 50 lines.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.36 seconds. Estimated time for incomplete lines: 51.68 seconds.\n",
            "2026-01-05 14:50:15 +0530   22800 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:15 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 59.67 seconds.\n",
            "2026-01-05 14:50:15 +0530   28448 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:15 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.6 seconds. Estimated time for incomplete lines: 62.4 seconds.\n",
            "2026-01-05 14:50:16 +0530   22800 execution.bulk     INFO     Finished 14 / 50 lines.\n",
            "2026-01-05 14:50:16 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.32 seconds. Estimated time for incomplete lines: 47.52 seconds.\n",
            "2026-01-05 14:50:17 +0530   24016 execution.bulk     INFO     Finished 17 / 50 lines.\n",
            "2026-01-05 14:50:17 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.12 seconds. Estimated time for incomplete lines: 36.96 seconds.\n",
            "2026-01-05 14:50:17 +0530   28448 execution.bulk     INFO     Finished 14 / 50 lines.\n",
            "2026-01-05 14:50:17 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.4 seconds. Estimated time for incomplete lines: 50.4 seconds.\n",
            "2026-01-05 14:50:18 +0530   24016 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:18 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 30.9 seconds.\n",
            "2026-01-05 14:50:20 +0530   22800 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:20 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 32.7 seconds.\n",
            "2026-01-05 14:50:21 +0530   28448 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.15 seconds. Estimated time for incomplete lines: 34.5 seconds.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 31.9 seconds.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Finished 22 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 29.96 seconds.\n",
            "2026-01-05 14:50:22 +0530   24016 execution.bulk     INFO     Finished 24 / 50 lines.\n",
            "2026-01-05 14:50:22 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.02 seconds. Estimated time for incomplete lines: 26.52 seconds.\n",
            "2026-01-05 14:50:23 +0530   22800 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:23 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.19 seconds. Estimated time for incomplete lines: 34.51 seconds.\n",
            "2026-01-05 14:50:24 +0530   28448 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:24 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.23 seconds. Estimated time for incomplete lines: 35.67 seconds.\n",
            "2026-01-05 14:50:25 +0530   28448 execution.bulk     INFO     Finished 23 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 31.59 seconds.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Finished 29 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 19.53 seconds.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 18.4 seconds.\n",
            "2026-01-05 14:50:26 +0530   28448 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:26 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 18.4 seconds.\n",
            "2026-01-05 14:50:26 +0530   22800 execution.bulk     INFO     Finished 26 / 50 lines.\n",
            "2026-01-05 14:50:26 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 25.68 seconds.\n",
            "2026-01-05 14:50:27 +0530   24016 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:27 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 18.05 seconds.\n",
            "2026-01-05 14:50:28 +0530   22800 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:28 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 20.0 seconds.\n",
            "2026-01-05 14:50:28 +0530   24016 execution.bulk     INFO     Finished 32 / 50 lines.\n",
            "2026-01-05 14:50:28 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 17.1 seconds.\n",
            "2026-01-05 14:50:29 +0530   24016 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 14:50:29 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 15.98 seconds.\n",
            "2026-01-05 14:50:30 +0530   24016 execution.bulk     INFO     Finished 36 / 50 lines.\n",
            "2026-01-05 14:50:30 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 12.46 seconds.\n",
            "2026-01-05 14:50:31 +0530   22800 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:31 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 20.33 seconds.\n",
            "2026-01-05 14:50:32 +0530   28448 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 20.9 seconds.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 8.6 seconds.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 7.47 seconds.\n",
            "2026-01-05 14:50:33 +0530   28448 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 14:50:33 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 18.19 seconds.\n",
            "2026-01-05 14:50:34 +0530   22800 execution.bulk     INFO     Finished 37 / 50 lines.\n",
            "2026-01-05 14:50:34 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 12.74 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 7.04 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 6.09 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 5.1 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 4.15 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.82 seconds. Estimated time for incomplete lines: 3.28 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.8 seconds. Estimated time for incomplete lines: 2.4 seconds.\n",
            "2026-01-05 14:50:36 +0530   22800 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 9.5 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.79 seconds. Estimated time for incomplete lines: 1.58 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.78 seconds. Estimated time for incomplete lines: 0.78 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Finished 39 / 50 lines.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 10.89 seconds.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 9.8 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"coherence_20260105_091958_326856\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.326856+00:00\"\n",
            "Duration: \"0:00:39.278192\"\n",
            "\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 9.0 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 7.84 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 6.65 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 5.58 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 4.55 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 3.56 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 2.64 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 1.72 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.84 seconds. Estimated time for incomplete lines: 0.84 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"fluency_20260105_091958_332852\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.332852+00:00\"\n",
            "Duration: \"0:00:42.034842\"\n",
            "\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 9.27 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 8.0 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 6.86 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 5.76 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 4.7 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 3.68 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 2.73 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 1.8 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 0.88 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"groundedness_20260105_091958_336895\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.336895+00:00\"\n",
            "Duration: \"0:00:43.737933\"\n",
            "\n",
            "======= Combined Run Summary (Per Evaluator) =======\n",
            "\n",
            "{\n",
            "    \"coherence\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:39.278192\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"fluency\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:42.034842\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"groundedness\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:43.737933\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    }\n",
            "}\n",
            "\n",
            "====================================================\n",
            "\n",
            "Evaluation results saved to \"C:\\work\\AMLRepos\\fine-tuning\\Demos\\DPO_Intel_Orca\\evaluation_results_gpt_4.1_mini_dpo_finetuned\".\n",
            "\n",
            "EVALUATION RESULTS: gpt-4.1-mini-dpo-finetuned\n",
            "\n",
            "Coherence:      3.8400 (1-5 scale)\n",
            "Fluency:        2.7000 (1-5 scale)\n",
            "Groundedness:   3.1000 (1-5 scale)\n",
            "============================================================\n",
            "Detailed results: ./evaluation_results_gpt_4.1_mini_dpo_finetuned\n",
            "\n",
            "Compare base model vs fine-tuned model metrics to see DPO improvements!\n"
          ]
        }
      ],
      "source": [
        "base_model = os.getenv(\"DEPLOYMENT_NAME\")  # Use base model for evaluation\n",
        "\n",
        "print(f\"Evaluating fine-tuned model: {deployment_name}\")\n",
        "\n",
        "finetuned_results = evaluate_model(deployment_name, num_samples=50, evaluator_model=base_model)\n",
        "\n",
        "print(\"\\nCompare base model vs fine-tuned model metrics to see DPO improvements!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.1 Model Comparison Results\n",
        "\n",
        "Below is an example comparison between base model and fine-tuned model evaluation results (using 50 samples):\n",
        "\n",
        "| Metric | Base Model | Fine-Tuned Model | Change | Status |\n",
        "|--------|-----------|------------------|--------|--------|\n",
        "| **Coherence** | 4.3000 | 3.8400 | -0.4600 | Decreased |\n",
        "| **Fluency** | 3.5800 | 2.7000 | -0.8800 | Decreased |\n",
        "| **Groundedness** | 4.1000 | 3.1000 | -1.0000 | Decreased |\n",
        "\n",
        "### Understanding the Results\n",
        "\n",
        "The example above shows that the fine-tuned model performed worse than the base model across all metrics. This indicates that the DPO training did not improve model quality with the current configuration.\n",
        "\n",
        "### How to Improve Results\n",
        "\n",
        "To achieve better fine-tuned model performance, experiment with:\n",
        "\n",
        "**1. Hyperparameter Tuning:**\n",
        "- **Reduce epochs**: Try `n_epochs=1` or `n_epochs=2` to prevent overfitting\n",
        "- **Lower learning rate**: Set `learning_rate_multiplier=0.5` or `0.1` for more conservative training\n",
        "- **Adjust batch size**: Keep at 1-2 for DPO stability\n",
        "\n",
        "**2. Training Data:**\n",
        "- **Increase sample size**: Use more training examples (e.g., 100-1000 samples)\n",
        "- **Verify data quality**: Ensure \"preferred_output\" responses are truly higher quality than rejected ones\n",
        "- **Review data format**: Confirm DPO pairs are correctly labeled\n",
        "\n",
        "**3. Evaluation Settings:**\n",
        "- **Increase evaluation samples**: Use `num_samples=100` or more for more reliable metrics\n",
        "- **Test on different data**: Evaluate on a separate validation set, not training data\n",
        "\n",
        "### Success Indicators\n",
        "\n",
        "Your fine-tuning is successful when you see **positive changes** like:\n",
        "- Coherence: +0.5 or higher\n",
        "- Fluency: +0.3 or higher  \n",
        "- Groundedness: +0.4 or higher\n",
        "\n",
        "Iterate on hyperparameters and training data until your fine-tuned model consistently outperforms the base model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Continual Fine-Tuning (Optional)\n",
        "\n",
        "If your fine-tuned model didn't show improvements, you can perform **continual fine-tuning** by using the fine-tuned model as the base for another round of training. This iterative approach can help refine the model further.\n",
        "\n",
        "### When to Use Continual Fine-Tuning:\n",
        "- Your first fine-tuning run didn't improve metrics\n",
        "- You want to adjust hyperparameters and train further\n",
        "- You have additional training data to incorporate\n",
        "- You need to fine-tune for a more specific task\n",
        "\n",
        "### How It Works:\n",
        "Instead of using model_name (base model), use fine_tuned_model_id from section 10 as your new base model. The code below is the same as section 8, but modified to continue training from your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Continual fine-tuning using base model: gpt-4.1-mini-2025-04-14.ft-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Continual fine-tuning job created!\n",
            "Job ID: ftjob-ca7805d53a5e496ca87aebca9894e134\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "\n",
        "continual_base_model = fine_tuned_model_id\n",
        "print(f\"Continual fine-tuning using base model: {continual_base_model}\")\n",
        "\n",
        "continual_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,  # You can use the same or upload new training data\n",
        "    validation_file=validation_file.id,\n",
        "    model=continual_base_model,  # Using fine-tuned model instead of base model\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 2,  # Reduced from 3 to prevent overfitting\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 0.5  # Lower learning rate for fine-tuning refinement\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\"Continual fine-tuning job created!\")\n",
        "print(f\"Job ID: {continual_job.id}\")\n",
        "print(f\"Status: {continual_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
