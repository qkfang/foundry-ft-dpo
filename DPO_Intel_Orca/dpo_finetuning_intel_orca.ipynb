{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Microsoft Foundry\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Microsoft Foundry\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model\n",
        "\n",
        "Note: Execute each cell in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-ai-projects>=2.0.0b1 (from -r requirements.txt (line 2))\n",
            "  Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting openai (from -r requirements.txt (line 5))\n",
            "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting azure-identity (from -r requirements.txt (line 8))\n",
            "  Using cached azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
            "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
            "  Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting azure-ai-evaluation>=1.13.0 (from -r requirements.txt (line 12))\n",
            "  Using cached azure_ai_evaluation-1.13.7-py3-none-any.whl.metadata (49 kB)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 15))\n",
            "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core>=1.35.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (4.15.0)\n",
            "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached azure_storage_blob-12.27.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Collecting sniffio (from openai->-r requirements.txt (line 5))\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting cryptography>=2.5 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
            "Collecting msal>=1.30.0 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting msrest>=0.7.1 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting azure-mgmt-core>=1.6.0 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyjwt>=2.8.0 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting nltk>=3.9.1 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas<3.0.0,>=2.1.2 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
            "Collecting ruamel.yaml<1.0.0,>=0.17.10 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached ruamel_yaml-0.19.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting Jinja2>=3.1.6 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting aiohttp>=3.0 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting requests>=2.21.0 (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.6->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
            "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting click (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "Collecting numpy>=1.23.2 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ruamel.yaml.clibz>=0.3.3 (from ruamel.yaml<1.0.0,>=0.17.10->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached ruamel_yaml_clibz-0.3.5-cp311-cp311-win_amd64.whl\n",
            "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
            "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Requirement already satisfied: six>=1.5 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.17.0)\n",
            "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl (234 kB)\n",
            "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "Using cached azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
            "Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
            "Using cached azure_ai_evaluation-1.13.7-py3-none-any.whl (1.1 MB)\n",
            "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
            "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "Using cached azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
            "Using cached azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
            "Using cached azure_storage_blob-12.27.1-py3-none-any.whl (428 kB)\n",
            "Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
            "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
            "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
            "Using cached ruamel_yaml-0.19.0-py3-none-any.whl (117 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
            "Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
            "Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
            "Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl (12.6 MB)\n",
            "Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
            "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Installing collected packages: pytz, urllib3, tzdata, typing-inspection, tqdm, sniffio, ruamel.yaml.clibz, regex, python-dotenv, pyjwt, pydantic-core, pycparser, propcache, oauthlib, numpy, multidict, MarkupSafe, joblib, jiter, isodate, idna, h11, frozenlist, distro, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, ruamel.yaml, requests, pydantic, pandas, nltk, Jinja2, httpcore, cffi, anyio, aiosignal, requests-oauthlib, httpx, cryptography, azure-core, aiohttp, openai, msrest, azure-storage-blob, azure-mgmt-core, msal, azure-mgmt-cognitiveservices, azure-ai-projects, msal-extensions, azure-identity, azure-ai-evaluation\n",
            "Successfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.12.0 attrs-25.4.0 azure-ai-evaluation-1.13.7 azure-ai-projects-2.0.0b2 azure-core-1.37.0 azure-identity-1.25.1 azure-mgmt-cognitiveservices-14.1.0 azure-mgmt-core-1.6.0 azure-storage-blob-12.27.1 certifi-2025.11.12 cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.1 cryptography-46.0.3 distro-1.9.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 isodate-0.7.2 jiter-0.12.0 joblib-1.5.3 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 multidict-6.7.0 nltk-3.9.2 numpy-2.4.0 oauthlib-3.3.1 openai-2.14.0 pandas-2.3.3 propcache-0.4.1 pycparser-2.23 pydantic-2.12.5 pydantic-core-2.41.5 pyjwt-2.10.1 python-dotenv-1.2.1 pytz-2025.2 regex-2025.11.3 requests-2.32.5 requests-oauthlib-2.0.0 ruamel.yaml-0.19.0 ruamel.yaml.clibz-0.3.5 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2 tzdata-2025.3 urllib3-2.6.2 yarl-1.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Evaluation Function\n",
        "\n",
        "Function to evaluate model performance using Azure AI Evaluation SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(deployment_name, num_samples=10, evaluator_model=None):\n",
        "    \"\"\"\n",
        "    Evaluate a model deployment using Azure AI Evaluation SDK.\n",
        "    \n",
        "    Args:\n",
        "        deployment_name: Name of the deployed model to evaluate\n",
        "        num_samples: Number of samples to evaluate (default: 10)\n",
        "        evaluator_model: Name of the model to use for evaluation (default: use base model from env)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from azure.ai.evaluation import evaluate, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
        "    from openai import AzureOpenAI\n",
        "    \n",
        "    if evaluator_model is None:\n",
        "        evaluator_model = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    \n",
        "    print(f\"Evaluating deployment: {deployment_name}\")\n",
        "    print(f\"Using evaluator model: {evaluator_model}\")\n",
        "    print(f\"Using {num_samples} samples from training.jsonl\")\n",
        "    \n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        api_version=\"2024-08-01-preview\"\n",
        "    )\n",
        "    \n",
        "    print(\"Generating model responses...\")\n",
        "    eval_data = []\n",
        "    with open(\"training.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            sample = json.loads(line)\n",
        "            \n",
        "            messages = sample[\"input\"][\"messages\"]\n",
        "            query = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "            \n",
        "            response = azure_openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            model_response = response.choices[0].message.content\n",
        "            \n",
        "            ground_truth = next((msg[\"content\"] for msg in sample[\"preferred_output\"] if msg[\"role\"] == \"assistant\"), \"\")\n",
        "            \n",
        "            eval_data.append({\n",
        "                \"query\": query,\n",
        "                \"response\": model_response,\n",
        "                \"ground_truth\": ground_truth\n",
        "            })\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == num_samples:\n",
        "                print(f\"  Processed {i+1}/{num_samples}\")\n",
        "    \n",
        "    eval_file = f\"evaluation_data_{deployment_name.replace('-', '_')}.jsonl\"\n",
        "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "        for item in eval_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    model_config = {\n",
        "        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        \"azure_deployment\": evaluator_model,\n",
        "        \"api_version\": \"2024-08-01-preview\",\n",
        "    }\n",
        "    \n",
        "    print(\"Running evaluation with 3 metrics...\")\n",
        "    try:\n",
        "        results = evaluate(\n",
        "            data=eval_file,\n",
        "            evaluators={\n",
        "                \"coherence\": CoherenceEvaluator(model_config=model_config),\n",
        "                \"fluency\": FluencyEvaluator(model_config=model_config),\n",
        "                \"groundedness\": GroundednessEvaluator(model_config=model_config)\n",
        "            },\n",
        "            evaluator_config={\n",
        "                \"default\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"ground_truth\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                },\n",
        "                \"groundedness\": {\n",
        "                    \"column_mapping\": {\n",
        "                        \"query\": \"${data.query}\",\n",
        "                        \"response\": \"${data.response}\",\n",
        "                        \"context\": \"${data.ground_truth}\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            output_path=f\"./evaluation_results_{deployment_name.replace('-', '_')}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation encountered an error: {str(e)}\")\n",
        "        print(\"Results may be incomplete. Check the output folder for partial results.\")\n",
        "        results = {\"metrics\": {}, \"error\": str(e)}\n",
        "    \n",
        "    print(f\"EVALUATION RESULTS: {deployment_name}\\n\")\n",
        "    \n",
        "    if \"metrics\" in results:\n",
        "        metrics = results[\"metrics\"]\n",
        "        \n",
        "        coherence = metrics.get('coherence.coherence', metrics.get('coherence'))\n",
        "        fluency = metrics.get('fluency.fluency', metrics.get('fluency'))\n",
        "        groundedness = metrics.get('groundedness.groundedness', metrics.get('groundedness'))\n",
        "        \n",
        "        if coherence is not None:\n",
        "            print(f\"Coherence:      {coherence:.4f} (1-5 scale)\")\n",
        "        if fluency is not None:\n",
        "            print(f\"Fluency:        {fluency:.4f} (1-5 scale)\")\n",
        "        if groundedness is not None:\n",
        "            print(f\"Groundedness:   {groundedness:.4f} (1-5 scale)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"Detailed results: ./evaluation_results_{deployment_name.replace('-', '_')}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure Azure Environment\n",
        "Set your Microsoft Foundry Project endpoint and model name. We're using **gpt-4.1-mini** in this example, but you can use other supported GPT models. Copy the file `.env.template` (located in this folder), and save it as file named `.env`. Enter appropriate values for the environment variables used for the job you want to run. \n",
        "\n",
        "```\n",
        "# Required for DPO Fine-Tuning\n",
        "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint> \n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "MODEL_NAME=<your-base-model-name>\n",
        "\n",
        "# Required for Model Local Evaluation\n",
        "AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>\n",
        "AZURE_OPENAI_KEY=<your-azure-openai-api-key>\n",
        "DEPLOYMENT_NAME=<your-deployment-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\"Base model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Microsoft Foundry Project\n",
        "\n",
        "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Microsoft Foundry Project\n"
          ]
        }
      ],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\"Connected to Microsoft Foundry Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading training file...\n",
            " Training file ID: file-4121fac45b5144bab840f2a8bea3eb9c\n",
            "\n",
            "Uploading validation file...\n",
            " Validation file ID: file-b3b53f48582b4bf89741bfbd1f6fd7a1\n"
          ]
        }
      ],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for files to be processed...\n",
            " Files ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Base Model\n",
        "\n",
        "Establish baseline performance metrics by evaluating the base model before DPO fine-tuning. This provides a comparison point to measure improvements after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating base model: gpt-4.1-mini\n",
            "\n",
            "Evaluating deployment: gpt-4.1-mini\n",
            "Using evaluator model: gpt-4.1-mini\n",
            "Using 50 samples from training.jsonl\n",
            "Generating model responses...\n",
            "  Processed 10/50\n",
            "  Processed 20/50\n",
            "  Processed 30/50\n",
            "  Processed 40/50\n",
            "  Processed 50/50\n",
            "Running evaluation with 3 metrics...\n",
            "2026-01-05 15:11:47 +0530   26352 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 15:11:47 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 8.13 seconds. Estimated time for incomplete lines: 398.37 seconds.\n",
            "2026-01-05 15:11:48 +0530   26352 execution.bulk     INFO     Finished 2 / 50 lines.\n",
            "2026-01-05 15:11:48 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 4.3 seconds. Estimated time for incomplete lines: 206.4 seconds.\n",
            "2026-01-05 15:11:48 +0530   14308 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 15:11:48 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 8.88 seconds. Estimated time for incomplete lines: 435.12 seconds.\n",
            "2026-01-05 15:11:49 +0530   29264 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 15:11:49 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 9.6 seconds. Estimated time for incomplete lines: 470.4 seconds.\n",
            "2026-01-05 15:11:49 +0530   26352 execution.bulk     INFO     Finished 5 / 50 lines.\n",
            "2026-01-05 15:11:49 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 2.04 seconds. Estimated time for incomplete lines: 91.8 seconds.\n",
            "2026-01-05 15:11:50 +0530   14308 execution.bulk     INFO     Finished 5 / 50 lines.\n",
            "2026-01-05 15:11:50 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 2.24 seconds. Estimated time for incomplete lines: 100.8 seconds.\n",
            "2026-01-05 15:11:51 +0530   29264 execution.bulk     INFO     Finished 4 / 50 lines.\n",
            "2026-01-05 15:11:51 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 2.9 seconds. Estimated time for incomplete lines: 133.4 seconds.\n",
            "2026-01-05 15:11:52 +0530   26352 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 15:11:52 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.29 seconds. Estimated time for incomplete lines: 51.6 seconds.\n",
            "2026-01-05 15:11:53 +0530   29264 execution.bulk     INFO     Finished 8 / 50 lines.\n",
            "2026-01-05 15:11:53 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.74 seconds. Estimated time for incomplete lines: 73.08 seconds.\n",
            "2026-01-05 15:11:53 +0530   14308 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 15:11:53 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.4 seconds. Estimated time for incomplete lines: 56.0 seconds.\n",
            "2026-01-05 15:11:54 +0530   29264 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 15:11:54 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.49 seconds. Estimated time for incomplete lines: 59.6 seconds.\n",
            "2026-01-05 15:11:55 +0530   26352 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 15:11:55 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.45 seconds. Estimated time for incomplete lines: 56.55 seconds.\n",
            "2026-01-05 15:11:56 +0530   14308 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 15:11:56 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 59.67 seconds.\n",
            "2026-01-05 15:11:56 +0530   26352 execution.bulk     INFO     Finished 14 / 50 lines.\n",
            "2026-01-05 15:11:56 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.24 seconds. Estimated time for incomplete lines: 44.64 seconds.\n",
            "2026-01-05 15:11:57 +0530   29264 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 15:11:57 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.63 seconds. Estimated time for incomplete lines: 63.57 seconds.\n",
            "2026-01-05 15:11:58 +0530   14308 execution.bulk     INFO     Finished 15 / 50 lines.\n",
            "2026-01-05 15:11:58 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 44.45 seconds.\n",
            "2026-01-05 15:11:58 +0530   29264 execution.bulk     INFO     Finished 13 / 50 lines.\n",
            "2026-01-05 15:11:58 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.48 seconds. Estimated time for incomplete lines: 54.76 seconds.\n",
            "2026-01-05 15:11:59 +0530   26352 execution.bulk     INFO     Finished 19 / 50 lines.\n",
            "2026-01-05 15:11:59 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 32.86 seconds.\n",
            "2026-01-05 15:12:00 +0530   26352 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 15:12:00 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 30.9 seconds.\n",
            "2026-01-05 15:12:01 +0530   29264 execution.bulk     INFO     Finished 17 / 50 lines.\n",
            "2026-01-05 15:12:01 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.28 seconds. Estimated time for incomplete lines: 42.24 seconds.\n",
            "2026-01-05 15:12:01 +0530   14308 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 15:12:01 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 32.7 seconds.\n",
            "2026-01-05 15:12:02 +0530   29264 execution.bulk     INFO     Finished 19 / 50 lines.\n",
            "2026-01-05 15:12:02 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.19 seconds. Estimated time for incomplete lines: 36.89 seconds.\n",
            "2026-01-05 15:12:02 +0530   26352 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 15:12:02 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.12 seconds. Estimated time for incomplete lines: 32.48 seconds.\n",
            "2026-01-05 15:12:03 +0530   29264 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 15:12:03 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.2 seconds. Estimated time for incomplete lines: 36.0 seconds.\n",
            "2026-01-05 15:12:03 +0530   26352 execution.bulk     INFO     Finished 23 / 50 lines.\n",
            "2026-01-05 15:12:03 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 28.62 seconds.\n",
            "2026-01-05 15:12:04 +0530   14308 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 15:12:04 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 34.22 seconds.\n",
            "2026-01-05 15:12:05 +0530   29264 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 15:12:05 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.22 seconds. Estimated time for incomplete lines: 35.38 seconds.\n",
            "2026-01-05 15:12:05 +0530   14308 execution.bulk     INFO     Finished 24 / 50 lines.\n",
            "2026-01-05 15:12:05 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 28.6 seconds.\n",
            "2026-01-05 15:12:06 +0530   29264 execution.bulk     INFO     Finished 23 / 50 lines.\n",
            "2026-01-05 15:12:06 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 31.59 seconds.\n",
            "2026-01-05 15:12:06 +0530   26352 execution.bulk     INFO     Finished 28 / 50 lines.\n",
            "2026-01-05 15:12:06 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 21.34 seconds.\n",
            "2026-01-05 15:12:07 +0530   14308 execution.bulk     INFO     Finished 26 / 50 lines.\n",
            "2026-01-05 15:12:07 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 25.68 seconds.\n",
            "2026-01-05 15:12:07 +0530   26352 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 15:12:07 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 18.8 seconds.\n",
            "2026-01-05 15:12:07 +0530   29264 execution.bulk     INFO     Finished 25 / 50 lines.\n",
            "2026-01-05 15:12:07 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 28.25 seconds.\n",
            "2026-01-05 15:12:09 +0530   14308 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 15:12:09 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 19.8 seconds.\n",
            "2026-01-05 15:12:09 +0530   29264 execution.bulk     INFO     Finished 28 / 50 lines.\n",
            "2026-01-05 15:12:09 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 23.54 seconds.\n",
            "2026-01-05 15:12:09 +0530   29264 execution.bulk     INFO     Finished 29 / 50 lines.\n",
            "2026-01-05 15:12:09 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 22.05 seconds.\n",
            "2026-01-05 15:12:10 +0530   29264 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 15:12:10 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 20.6 seconds.\n",
            "2026-01-05 15:12:10 +0530   26352 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 15:12:10 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 1.01 seconds. Estimated time for incomplete lines: 19.19 seconds.\n",
            "2026-01-05 15:12:11 +0530   26352 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 15:12:11 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 16.66 seconds.\n",
            "2026-01-05 15:12:12 +0530   14308 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 15:12:12 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 19.95 seconds.\n",
            "2026-01-05 15:12:13 +0530   29264 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 15:12:13 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 20.71 seconds.\n",
            "2026-01-05 15:12:13 +0530   14308 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 15:12:13 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.02 seconds. Estimated time for incomplete lines: 17.34 seconds.\n",
            "2026-01-05 15:12:13 +0530   26352 execution.bulk     INFO     Finished 36 / 50 lines.\n",
            "2026-01-05 15:12:13 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 13.16 seconds.\n",
            "2026-01-05 15:12:13 +0530   14308 execution.bulk     INFO     Finished 34 / 50 lines.\n",
            "2026-01-05 15:12:13 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 1.01 seconds. Estimated time for incomplete lines: 16.16 seconds.\n",
            "2026-01-05 15:12:15 +0530   29264 execution.bulk     INFO     Finished 34 / 50 lines.\n",
            "2026-01-05 15:12:15 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 16.8 seconds.\n",
            "2026-01-05 15:12:15 +0530   26352 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 15:12:15 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 9.1 seconds.\n",
            "2026-01-05 15:12:17 +0530   14308 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 15:12:17 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 9.4 seconds.\n",
            "2026-01-05 15:12:17 +0530   29264 execution.bulk     INFO     Finished 39 / 50 lines.\n",
            "2026-01-05 15:12:17 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 10.89 seconds.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 8.46 seconds.\n",
            "2026-01-05 15:12:18 +0530   29264 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 15:12:18 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 9.7 seconds.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 7.44 seconds.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 15:12:18 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 6.44 seconds.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 5.4 seconds.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 4.4 seconds.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 3.48 seconds.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 2.55 seconds.\n",
            "2026-01-05 15:12:19 +0530   14308 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 8.82 seconds.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 15:12:19 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.84 seconds. Estimated time for incomplete lines: 1.68 seconds.\n",
            "2026-01-05 15:12:20 +0530   26352 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.83 seconds.\n",
            "2026-01-05 15:12:20 +0530   26352 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   26352 execution.bulk     INFO     Average execution time for completed lines: 0.81 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 7.76 seconds.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 6.65 seconds.\n",
            "2026-01-05 15:12:20 +0530   29264 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 9.0 seconds.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 5.64 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"coherence_20260105_094139_490349\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:41:39.490349+00:00\"\n",
            "Duration: \"0:00:41.240094\"\n",
            "\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 4.6 seconds.\n",
            "2026-01-05 15:12:20 +0530   29264 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 15:12:20 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 7.92 seconds.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 3.6 seconds.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 2.67 seconds.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 1.74 seconds.\n",
            "2026-01-05 15:12:21 +0530   29264 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 15:12:21 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 6.79 seconds.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 15:12:21 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 0.85 seconds.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 5.82 seconds.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 4.75 seconds.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 3.76 seconds.\n",
            "2026-01-05 15:12:22 +0530   14308 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 15:12:22 +0530   14308 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 15:12:22 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 2.76 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"fluency_20260105_094139_492329\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:41:39.492329+00:00\"\n",
            "Duration: \"0:00:43.676952\"\n",
            "\n",
            "2026-01-05 15:12:23 +0530   29264 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 15:12:23 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 1.82 seconds.\n",
            "2026-01-05 15:12:23 +0530   29264 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 15:12:23 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 0.9 seconds.\n",
            "2026-01-05 15:12:24 +0530   29264 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 15:12:24 +0530   29264 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"groundedness_20260105_094139_499354\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:41:39.499354+00:00\"\n",
            "Duration: \"0:00:45.497860\"\n",
            "\n",
            "======= Combined Run Summary (Per Evaluator) =======\n",
            "\n",
            "{\n",
            "    \"coherence\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:41.240094\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"fluency\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:43.676952\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"groundedness\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:45.497860\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    }\n",
            "}\n",
            "\n",
            "====================================================\n",
            "\n",
            "Evaluation results saved to \"C:\\work\\AMLRepos\\fine-tuning\\Demos\\DPO_Intel_Orca\\evaluation_results_gpt_4.1_mini\".\n",
            "\n",
            "EVALUATION RESULTS: gpt-4.1-mini\n",
            "\n",
            "Coherence:      4.3000 (1-5 scale)\n",
            "Fluency:        3.5800 (1-5 scale)\n",
            "Groundedness:   4.1000 (1-5 scale)\n",
            "============================================================\n",
            "Detailed results: ./evaluation_results_gpt_4.1_mini\n"
          ]
        }
      ],
      "source": [
        "base_deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "print(f\"Evaluating base model: {base_deployment}\\n\")\n",
        "\n",
        "base_results = evaluate_model(base_deployment, num_samples=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Job ID: ftjob-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 3,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: succeeded\n"
          ]
        }
      ],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tokens billed: 7223000\n",
            "Completed results file: file-697f5d7ff2b14ced8258a9c152973400\n",
            "Model Evaluation Passed.\n",
            "Job succeeded.\n",
            "Step 8250: training loss=3.814697265625E-06\n",
            "Step 8240: training loss=3.814697265625E-06\n",
            "Step 8230: training loss=4.76837158203125E-06\n",
            "Step 8220: training loss=5.7220458984375E-06\n",
            "Step 8210: training loss=4.76837158203125E-06\n",
            "Step 8200: training loss=3.814697265625E-06\n",
            "Step 8190: training loss=6.67572021484375E-06\n",
            "Step 8180: training loss=3.814697265625E-06\n",
            "Step 8170: training loss=4.76837158203125E-06\n",
            "Step 8160: training loss=0.007175922393798828\n",
            "Step 8150: training loss=0.0005140304565429688\n",
            "Step 8140: training loss=4.76837158203125E-06\n",
            "Step 8130: training loss=3.814697265625E-06\n",
            "Step 8120: training loss=3.814697265625E-06\n",
            "Step 8110: training loss=6.67572021484375E-06\n",
            "Step 8100: training loss=4.76837158203125E-06\n",
            "Step 8090: training loss=4.76837158203125E-06\n",
            "Step 8080: training loss=4.76837158203125E-06\n",
            "Step 8070: training loss=3.814697265625E-06\n",
            "Step 8060: training loss=4.76837158203125E-06\n",
            "Step 8050: training loss=4.76837158203125E-06\n",
            "Step 8040: training loss=6.67572021484375E-06\n",
            "Step 8030: training loss=4.76837158203125E-06\n",
            "Step 8020: training loss=5.7220458984375E-06\n",
            "Step 8010: training loss=4.76837158203125E-06\n",
            "Step 8000: training loss=3.814697265625E-06\n",
            "Step 7990: training loss=3.814697265625E-06\n",
            "Step 7980: training loss=5.7220458984375E-06\n",
            "Step 7970: training loss=3.814697265625E-06\n",
            "Step 7960: training loss=5.7220458984375E-06\n",
            "Step 7950: training loss=3.814697265625E-06\n",
            "Step 7940: training loss=3.814697265625E-06\n",
            "Step 7930: training loss=3.814697265625E-06\n",
            "Step 7920: training loss=3.814697265625E-06\n",
            "Step 7910: training loss=3.814697265625E-06\n",
            "Step 7900: training loss=4.76837158203125E-06\n",
            "Step 7890: training loss=6.67572021484375E-06\n",
            "Step 7880: training loss=2.86102294921875E-06\n",
            "Step 7870: training loss=3.814697265625E-06\n",
            "Step 7860: training loss=6.67572021484375E-06\n",
            "Step 7850: training loss=3.814697265625E-06\n",
            "Step 7840: training loss=6.67572021484375E-06\n",
            "Step 7830: training loss=3.814697265625E-06\n",
            "Step 7820: training loss=3.814697265625E-06\n",
            "Step 7810: training loss=4.76837158203125E-06\n",
            "Step 7800: training loss=4.76837158203125E-06\n",
            "Step 7790: training loss=4.76837158203125E-06\n",
            "Step 7780: training loss=4.76837158203125E-06\n",
            "Step 7770: training loss=8.58306884765625E-06\n",
            "Step 7760: training loss=4.76837158203125E-06\n",
            "Step 7750: training loss=4.76837158203125E-06\n",
            "Step 7740: training loss=8.58306884765625E-06\n",
            "Step 7730: training loss=3.814697265625E-06\n",
            "Step 7720: training loss=5.7220458984375E-06\n",
            "Step 7710: training loss=4.76837158203125E-06\n",
            "Step 7700: training loss=4.76837158203125E-06\n",
            "Step 7690: training loss=2.86102294921875E-06\n",
            "Step 7680: training loss=3.814697265625E-06\n",
            "Step 7670: training loss=5.7220458984375E-06\n",
            "Step 7660: training loss=4.76837158203125E-06\n",
            "Step 7650: training loss=3.814697265625E-06\n",
            "Step 7640: training loss=3.814697265625E-06\n",
            "Step 7630: training loss=3.814697265625E-06\n",
            "Step 7620: training loss=3.814697265625E-06\n",
            "Step 7610: training loss=3.814697265625E-06\n",
            "Step 7600: training loss=3.814697265625E-06\n",
            "Step 7590: training loss=4.76837158203125E-06\n",
            "Step 7580: training loss=5.7220458984375E-06\n",
            "Step 7570: training loss=5.7220458984375E-06\n",
            "Step 7560: training loss=4.76837158203125E-06\n",
            "Step 7550: training loss=3.814697265625E-06\n",
            "Step 7540: training loss=0.09464693069458008\n",
            "Step 7530: training loss=2.86102294921875E-06\n",
            "Step 7520: training loss=5.7220458984375E-06\n",
            "Step 7510: training loss=4.76837158203125E-06\n",
            "Step 7500: training loss=4.76837158203125E-06\n",
            "Step 7490: training loss=5.7220458984375E-06\n",
            "Step 7480: training loss=3.814697265625E-06\n",
            "Step 7470: training loss=4.76837158203125E-06\n",
            "Step 7460: training loss=4.76837158203125E-06\n",
            "Step 7450: training loss=5.7220458984375E-06\n",
            "Step 7440: training loss=0.00023555755615234375\n",
            "Step 7430: training loss=5.7220458984375E-06\n",
            "Step 7420: training loss=2.86102294921875E-06\n",
            "Step 7410: training loss=8.58306884765625E-06\n",
            "Step 7400: training loss=3.814697265625E-06\n",
            "Step 7390: training loss=3.814697265625E-06\n",
            "Step 7380: training loss=4.76837158203125E-06\n",
            "Step 7370: training loss=4.76837158203125E-06\n",
            "Step 7360: training loss=0.22778987884521484\n",
            "Step 7350: training loss=3.814697265625E-06\n",
            "Step 7340: training loss=8.58306884765625E-06\n",
            "Step 7330: training loss=5.7220458984375E-06\n",
            "Step 7320: training loss=4.76837158203125E-06\n",
            "Step 7310: training loss=3.814697265625E-06\n",
            "Step 7300: training loss=7.62939453125E-06\n",
            "Step 7290: training loss=4.76837158203125E-06\n",
            "Step 7280: training loss=4.76837158203125E-06\n",
            "Step 7270: training loss=2.86102294921875E-06\n",
            "Step 7260: training loss=3.814697265625E-06\n",
            "Step 7250: training loss=1.049041748046875E-05\n",
            "Step 7240: training loss=2.86102294921875E-06\n",
            "Step 7230: training loss=4.76837158203125E-06\n",
            "Step 7220: training loss=4.76837158203125E-06\n",
            "Step 7210: training loss=4.76837158203125E-06\n",
            "Step 7200: training loss=5.7220458984375E-06\n",
            "Step 7190: training loss=3.814697265625E-06\n",
            "Step 7180: training loss=4.76837158203125E-06\n",
            "Step 7170: training loss=4.76837158203125E-06\n",
            "Step 7160: training loss=1.049041748046875E-05\n",
            "Step 7150: training loss=5.7220458984375E-06\n",
            "Step 7140: training loss=6.67572021484375E-06\n",
            "Step 7130: training loss=4.76837158203125E-06\n",
            "Step 7120: training loss=2.86102294921875E-06\n",
            "Step 7110: training loss=3.814697265625E-06\n",
            "Step 7100: training loss=4.76837158203125E-06\n",
            "Step 7090: training loss=3.814697265625E-06\n",
            "Step 7080: training loss=4.76837158203125E-06\n",
            "Step 7070: training loss=4.76837158203125E-06\n",
            "Step 7060: training loss=3.814697265625E-06\n",
            "Step 7050: training loss=5.7220458984375E-06\n",
            "Step 7040: training loss=0.0002689361572265625\n",
            "Step 7030: training loss=4.76837158203125E-06\n",
            "Step 7020: training loss=4.76837158203125E-06\n",
            "Step 7010: training loss=3.814697265625E-06\n",
            "Step 7000: training loss=4.76837158203125E-06\n",
            "Step 6990: training loss=3.814697265625E-06\n",
            "Step 6980: training loss=3.814697265625E-06\n",
            "Step 6970: training loss=6.67572021484375E-06\n",
            "Step 6960: training loss=7.62939453125E-06\n",
            "Step 6950: training loss=6.67572021484375E-06\n",
            "Step 6940: training loss=4.76837158203125E-06\n",
            "Step 6930: training loss=2.86102294921875E-06\n",
            "Step 6920: training loss=4.76837158203125E-06\n",
            "Step 6910: training loss=3.814697265625E-06\n",
            "Step 6900: training loss=1.430511474609375E-05\n",
            "Step 6890: training loss=6.67572021484375E-06\n",
            "Step 6880: training loss=7.62939453125E-06\n",
            "Step 6870: training loss=4.76837158203125E-06\n",
            "Step 6860: training loss=4.76837158203125E-06\n",
            "Step 6850: training loss=6.67572021484375E-06\n",
            "Step 6840: training loss=0.0006723403930664062\n",
            "Step 6830: training loss=3.814697265625E-06\n",
            "Step 6820: training loss=4.76837158203125E-06\n",
            "Step 6810: training loss=8.58306884765625E-06\n",
            "Step 6800: training loss=3.814697265625E-06\n",
            "Step 6790: training loss=4.76837158203125E-06\n",
            "Step 6780: training loss=5.7220458984375E-06\n",
            "Step 6770: training loss=5.7220458984375E-06\n",
            "Step 6760: training loss=3.814697265625E-06\n",
            "Step 6750: training loss=3.814697265625E-06\n",
            "Step 6740: training loss=4.76837158203125E-06\n",
            "Step 6730: training loss=3.814697265625E-06\n",
            "Step 6720: training loss=4.76837158203125E-06\n",
            "Step 6710: training loss=8.58306884765625E-06\n",
            "Step 6700: training loss=5.7220458984375E-06\n",
            "Step 6690: training loss=6.67572021484375E-06\n",
            "Step 6680: training loss=4.76837158203125E-06\n",
            "Step 6670: training loss=3.814697265625E-06\n",
            "Step 6660: training loss=1.049041748046875E-05\n",
            "Step 6650: training loss=1.430511474609375E-05\n",
            "Step 6640: training loss=5.7220458984375E-06\n",
            "Step 6630: training loss=4.76837158203125E-06\n",
            "Step 6620: training loss=3.814697265625E-06\n",
            "Step 6610: training loss=5.7220458984375E-06\n",
            "Step 6600: training loss=4.76837158203125E-06\n",
            "Step 6590: training loss=0.51633220911026\n",
            "Step 6580: training loss=4.76837158203125E-06\n",
            "Step 6570: training loss=4.76837158203125E-06\n",
            "Step 6560: training loss=5.7220458984375E-06\n",
            "Step 6550: training loss=8.58306884765625E-06\n",
            "Step 6540: training loss=0.01696014404296875\n",
            "Step 6530: training loss=6.67572021484375E-06\n",
            "Step 6520: training loss=3.814697265625E-06\n",
            "Step 6510: training loss=4.76837158203125E-06\n",
            "Step 6500: training loss=3.814697265625E-06\n",
            "Step 6490: training loss=5.7220458984375E-06\n",
            "Step 6480: training loss=5.7220458984375E-06\n",
            "Step 6470: training loss=3.814697265625E-06\n",
            "Step 6460: training loss=4.76837158203125E-06\n",
            "Step 6450: training loss=3.814697265625E-06\n",
            "Step 6440: training loss=0.0285646915435791\n",
            "Step 6430: training loss=7.62939453125E-06\n",
            "Step 6420: training loss=4.76837158203125E-06\n",
            "Step 6410: training loss=5.7220458984375E-06\n",
            "Step 6400: training loss=4.76837158203125E-06\n",
            "Step 6390: training loss=3.814697265625E-06\n",
            "Step 6380: training loss=5.7220458984375E-06\n",
            "Step 6370: training loss=5.7220458984375E-06\n",
            "Step 6360: training loss=3.814697265625E-06\n",
            "Step 6350: training loss=4.76837158203125E-06\n",
            "Step 6340: training loss=4.76837158203125E-06\n",
            "Step 6330: training loss=7.724761962890625E-05\n",
            "Step 6320: training loss=4.76837158203125E-06\n",
            "Step 6310: training loss=3.814697265625E-06\n",
            "Step 6300: training loss=4.76837158203125E-06\n",
            "Step 6290: training loss=3.814697265625E-06\n",
            "Step 6280: training loss=4.76837158203125E-06\n",
            "Step 6270: training loss=3.814697265625E-06\n",
            "Step 6260: training loss=5.7220458984375E-06\n",
            "Step 6250: training loss=4.76837158203125E-06\n",
            "Step 6240: training loss=7.62939453125E-06\n",
            "Step 6230: training loss=5.7220458984375E-06\n",
            "Step 6220: training loss=4.76837158203125E-06\n",
            "Step 6210: training loss=3.814697265625E-06\n",
            "Step 6200: training loss=3.814697265625E-06\n",
            "Step 6190: training loss=3.814697265625E-06\n",
            "Step 6180: training loss=4.76837158203125E-06\n",
            "Step 6170: training loss=5.7220458984375E-06\n",
            "Step 6160: training loss=4.76837158203125E-06\n",
            "Step 6150: training loss=0.00025177001953125\n",
            "Step 6140: training loss=4.76837158203125E-06\n",
            "Step 6130: training loss=5.7220458984375E-06\n",
            "Step 6120: training loss=9.5367431640625E-06\n",
            "Step 6110: training loss=4.76837158203125E-06\n",
            "Step 6100: training loss=4.76837158203125E-06\n",
            "Step 6090: training loss=3.814697265625E-06\n",
            "Step 6080: training loss=0.0009722709655761719\n",
            "Step 6070: training loss=0.000247955322265625\n",
            "Step 6060: training loss=0.2191091775894165\n",
            "Step 6050: training loss=0.0003147125244140625\n",
            "Step 6040: training loss=6.67572021484375E-06\n",
            "Step 6030: training loss=3.814697265625E-06\n",
            "Step 6020: training loss=5.7220458984375E-06\n",
            "Step 6010: training loss=4.76837158203125E-06\n",
            "Step 6000: training loss=9.5367431640625E-06\n",
            "Step 5990: training loss=6.67572021484375E-06\n",
            "Step 5980: training loss=3.814697265625E-06\n",
            "Step 5970: training loss=3.814697265625E-06\n",
            "Step 5960: training loss=5.7220458984375E-06\n",
            "Step 5950: training loss=4.76837158203125E-06\n",
            "Step 5940: training loss=3.814697265625E-06\n",
            "Step 5930: training loss=6.67572021484375E-06\n",
            "Step 5920: training loss=0.26009881496429443\n",
            "Step 5910: training loss=5.7220458984375E-06\n",
            "Step 5900: training loss=4.76837158203125E-06\n",
            "Step 5890: training loss=0.00024318695068359375\n",
            "Step 5880: training loss=5.7220458984375E-06\n",
            "Step 5870: training loss=7.62939453125E-06\n",
            "Step 5860: training loss=4.76837158203125E-06\n",
            "Step 5850: training loss=4.76837158203125E-06\n",
            "Step 5840: training loss=2.86102294921875E-06\n",
            "Step 5830: training loss=3.814697265625E-06\n",
            "Step 5820: training loss=0.00018978118896484375\n",
            "Step 5810: training loss=4.76837158203125E-06\n",
            "Step 5800: training loss=6.67572021484375E-06\n",
            "Step 5790: training loss=2.09808349609375E-05\n",
            "Step 5780: training loss=3.814697265625E-06\n",
            "Step 5770: training loss=4.76837158203125E-06\n",
            "Step 5760: training loss=6.67572021484375E-06\n",
            "Step 5750: training loss=3.814697265625E-06\n",
            "Step 5740: training loss=3.814697265625E-06\n",
            "Step 5730: training loss=4.76837158203125E-06\n",
            "Step 5720: training loss=0.002689361572265625\n",
            "Step 5710: training loss=6.67572021484375E-06\n",
            "Step 5700: training loss=6.67572021484375E-06\n",
            "Step 5690: training loss=1.049041748046875E-05\n",
            "Step 5680: training loss=5.7220458984375E-06\n",
            "Step 5670: training loss=2.86102294921875E-06\n",
            "Step 5660: training loss=3.814697265625E-06\n",
            "Step 5650: training loss=3.814697265625E-06\n",
            "Step 5640: training loss=3.814697265625E-06\n",
            "Step 5630: training loss=4.76837158203125E-06\n",
            "Step 5620: training loss=4.76837158203125E-06\n",
            "Step 5610: training loss=5.7220458984375E-06\n",
            "Step 5600: training loss=6.67572021484375E-06\n",
            "Step 5590: training loss=4.76837158203125E-06\n",
            "Step 5580: training loss=3.814697265625E-06\n",
            "Step 5570: training loss=4.76837158203125E-06\n",
            "Step 5560: training loss=4.76837158203125E-06\n",
            "Step 5550: training loss=4.76837158203125E-06\n",
            "Step 5540: training loss=3.814697265625E-06\n",
            "Step 5530: training loss=7.62939453125E-06\n",
            "Step 5520: training loss=4.76837158203125E-06\n",
            "Step 5510: training loss=3.814697265625E-06\n",
            "Step 5500: training loss=4.76837158203125E-06\n",
            "Step 5490: training loss=2.86102294921875E-06\n",
            "Step 5480: training loss=6.67572021484375E-06\n",
            "Step 5470: training loss=3.814697265625E-06\n",
            "Step 5460: training loss=4.76837158203125E-06\n",
            "Step 5450: training loss=3.814697265625E-06\n",
            "Step 5440: training loss=5.7220458984375E-06\n",
            "Step 5430: training loss=3.814697265625E-06\n",
            "Step 5420: training loss=3.814697265625E-06\n",
            "Step 5410: training loss=0.027644872665405273\n",
            "Step 5400: training loss=0.0003986358642578125\n",
            "Step 5390: training loss=7.62939453125E-06\n",
            "Step 5380: training loss=3.814697265625E-06\n",
            "Step 5370: training loss=4.76837158203125E-06\n",
            "Step 5360: training loss=0.5506751537322998\n",
            "Step 5350: training loss=3.814697265625E-06\n",
            "Step 5340: training loss=0.3897521495819092\n",
            "Step 5330: training loss=4.76837158203125E-06\n",
            "Step 5320: training loss=5.7220458984375E-06\n",
            "Step 5310: training loss=7.62939453125E-06\n",
            "Step 5300: training loss=6.67572021484375E-06\n",
            "Step 5290: training loss=6.67572021484375E-06\n",
            "Step 5280: training loss=0.00030422210693359375\n",
            "Step 5270: training loss=4.76837158203125E-06\n",
            "Step 5260: training loss=4.76837158203125E-06\n",
            "Step 5250: training loss=6.67572021484375E-06\n",
            "Step 5240: training loss=4.76837158203125E-06\n",
            "Step 5230: training loss=3.814697265625E-06\n",
            "Step 5220: training loss=5.7220458984375E-06\n",
            "Step 5210: training loss=7.62939453125E-06\n",
            "Step 5200: training loss=4.76837158203125E-06\n",
            "Step 5190: training loss=4.76837158203125E-06\n",
            "Step 5180: training loss=5.7220458984375E-06\n",
            "Step 5170: training loss=3.814697265625E-06\n",
            "Step 5160: training loss=4.76837158203125E-06\n",
            "Step 5150: training loss=0.3023488521575928\n",
            "Step 5140: training loss=4.76837158203125E-06\n",
            "Step 5130: training loss=5.7220458984375E-06\n",
            "Step 5120: training loss=4.76837158203125E-06\n",
            "Step 5110: training loss=5.7220458984375E-06\n",
            "Step 5100: training loss=3.814697265625E-06\n",
            "Step 5090: training loss=5.7220458984375E-06\n",
            "Step 5080: training loss=3.814697265625E-06\n",
            "Step 5070: training loss=4.76837158203125E-06\n",
            "Step 5060: training loss=3.814697265625E-06\n",
            "Step 5050: training loss=1.239776611328125E-05\n",
            "Step 5040: training loss=8.58306884765625E-06\n",
            "Step 5030: training loss=5.7220458984375E-06\n",
            "Step 5020: training loss=3.814697265625E-06\n",
            "Step 5010: training loss=2.86102294921875E-06\n",
            "Step 5000: training loss=8.58306884765625E-06\n",
            "Step 4990: training loss=7.62939453125E-06\n",
            "Step 4980: training loss=3.814697265625E-06\n",
            "Step 4970: training loss=4.76837158203125E-06\n",
            "Step 4960: training loss=3.814697265625E-06\n",
            "Step 4950: training loss=3.814697265625E-06\n",
            "Step 4940: training loss=2.47955322265625E-05\n",
            "Step 4930: training loss=5.7220458984375E-06\n",
            "Step 4920: training loss=3.814697265625E-06\n",
            "Step 4910: training loss=3.814697265625E-06\n",
            "Step 4900: training loss=5.7220458984375E-06\n",
            "Step 4890: training loss=4.76837158203125E-06\n",
            "Step 4880: training loss=6.67572021484375E-06\n",
            "Step 4870: training loss=5.7220458984375E-06\n",
            "Step 4860: training loss=4.76837158203125E-06\n",
            "Step 4850: training loss=6.67572021484375E-06\n",
            "Step 4840: training loss=6.67572021484375E-06\n",
            "Step 4830: training loss=3.337860107421875E-05\n",
            "Step 4820: training loss=5.7220458984375E-06\n",
            "Step 4810: training loss=4.76837158203125E-06\n",
            "Step 4800: training loss=6.67572021484375E-06\n",
            "Step 4790: training loss=4.76837158203125E-06\n",
            "Step 4780: training loss=0.01096963882446289\n",
            "Step 4770: training loss=5.7220458984375E-06\n",
            "Step 4760: training loss=3.814697265625E-06\n",
            "Step 4750: training loss=8.58306884765625E-06\n",
            "Step 4740: training loss=4.76837158203125E-06\n",
            "Step 4730: training loss=4.76837158203125E-06\n",
            "Step 4720: training loss=4.76837158203125E-06\n",
            "Step 4710: training loss=6.29425048828125E-05\n",
            "Step 4700: training loss=8.58306884765625E-06\n",
            "Step 4690: training loss=5.7220458984375E-06\n",
            "Step 4680: training loss=4.76837158203125E-06\n",
            "Step 4670: training loss=4.76837158203125E-06\n",
            "Step 4660: training loss=3.814697265625E-06\n",
            "Step 4650: training loss=4.76837158203125E-06\n",
            "Step 4640: training loss=5.7220458984375E-06\n",
            "Step 4630: training loss=5.7220458984375E-06\n",
            "Step 4620: training loss=2.86102294921875E-06\n",
            "Step 4610: training loss=6.67572021484375E-06\n",
            "Step 4600: training loss=6.67572021484375E-06\n",
            "Step 4590: training loss=8.58306884765625E-06\n",
            "Step 4580: training loss=4.76837158203125E-06\n",
            "Step 4570: training loss=4.76837158203125E-06\n",
            "Step 4560: training loss=3.814697265625E-06\n",
            "Step 4550: training loss=5.7220458984375E-06\n",
            "Step 4540: training loss=4.76837158203125E-06\n",
            "Step 4530: training loss=5.7220458984375E-06\n",
            "Step 4520: training loss=4.76837158203125E-06\n",
            "Step 4510: training loss=5.7220458984375E-06\n",
            "Step 4500: training loss=7.62939453125E-06\n",
            "Step 4490: training loss=5.7220458984375E-06\n",
            "Step 4480: training loss=4.76837158203125E-06\n",
            "Step 4470: training loss=4.76837158203125E-06\n",
            "Step 4460: training loss=5.7220458984375E-06\n",
            "Step 4450: training loss=4.76837158203125E-06\n",
            "Step 4440: training loss=4.76837158203125E-06\n",
            "Step 4430: training loss=6.67572021484375E-06\n",
            "Step 4420: training loss=5.7220458984375E-06\n",
            "Step 4410: training loss=3.814697265625E-06\n",
            "Step 4400: training loss=3.814697265625E-06\n",
            "Step 4390: training loss=5.7220458984375E-06\n",
            "Step 4380: training loss=5.7220458984375E-06\n",
            "Step 4370: training loss=4.76837158203125E-06\n",
            "Step 4360: training loss=5.7220458984375E-06\n",
            "Step 4350: training loss=4.76837158203125E-06\n",
            "Step 4340: training loss=3.814697265625E-06\n",
            "Step 4330: training loss=5.7220458984375E-06\n",
            "Step 4320: training loss=1.52587890625E-05\n",
            "Step 4310: training loss=4.76837158203125E-06\n",
            "Step 4300: training loss=5.7220458984375E-06\n",
            "Step 4290: training loss=6.67572021484375E-06\n",
            "Step 4280: training loss=7.62939453125E-06\n",
            "Step 4270: training loss=5.7220458984375E-06\n",
            "Step 4260: training loss=4.76837158203125E-06\n",
            "Step 4250: training loss=5.7220458984375E-06\n",
            "Step 4240: training loss=4.76837158203125E-06\n",
            "Step 4230: training loss=9.5367431640625E-06\n",
            "Step 4220: training loss=6.67572021484375E-06\n",
            "Step 4210: training loss=3.814697265625E-06\n",
            "Step 4200: training loss=4.76837158203125E-06\n",
            "Step 4190: training loss=3.814697265625E-06\n",
            "Step 4180: training loss=4.76837158203125E-06\n",
            "Step 4170: training loss=0.6490526795387268\n",
            "Step 4160: training loss=4.76837158203125E-06\n",
            "Step 4150: training loss=5.7220458984375E-06\n",
            "Step 4140: training loss=9.5367431640625E-06\n",
            "Step 4130: training loss=2.86102294921875E-06\n",
            "Step 4120: training loss=5.7220458984375E-06\n",
            "Step 4110: training loss=8.58306884765625E-06\n",
            "Step 4100: training loss=4.76837158203125E-06\n",
            "Step 4090: training loss=4.76837158203125E-06\n",
            "Step 4080: training loss=7.62939453125E-06\n",
            "Step 4070: training loss=6.67572021484375E-06\n",
            "Step 4060: training loss=4.76837158203125E-06\n",
            "Step 4050: training loss=6.67572021484375E-06\n",
            "Step 4040: training loss=5.7220458984375E-06\n",
            "Step 4030: training loss=3.814697265625E-06\n",
            "Step 4020: training loss=0.0004496574401855469\n",
            "Step 4010: training loss=4.76837158203125E-06\n",
            "Step 4000: training loss=4.76837158203125E-06\n",
            "Step 3990: training loss=5.7220458984375E-06\n",
            "Step 3980: training loss=0.036392927169799805\n",
            "Step 3970: training loss=9.5367431640625E-06\n",
            "Step 3960: training loss=5.7220458984375E-06\n",
            "Step 3950: training loss=1.33514404296875E-05\n",
            "Step 3940: training loss=4.76837158203125E-06\n",
            "Step 3930: training loss=1.71661376953125E-05\n",
            "Step 3920: training loss=5.7220458984375E-06\n",
            "Step 3910: training loss=8.58306884765625E-06\n",
            "Step 3900: training loss=4.76837158203125E-06\n",
            "Step 3890: training loss=7.62939453125E-06\n",
            "Step 3880: training loss=0.00014209747314453125\n",
            "Step 3870: training loss=5.7220458984375E-06\n",
            "Step 3860: training loss=0.0012760162353515625\n",
            "Step 3850: training loss=6.67572021484375E-06\n",
            "Step 3840: training loss=4.76837158203125E-06\n",
            "Step 3830: training loss=5.7220458984375E-06\n",
            "Step 3820: training loss=4.76837158203125E-06\n",
            "Step 3810: training loss=4.76837158203125E-06\n",
            "Step 3800: training loss=6.67572021484375E-06\n",
            "Step 3790: training loss=5.7220458984375E-06\n",
            "Step 3780: training loss=3.814697265625E-06\n",
            "Step 3770: training loss=6.67572021484375E-06\n",
            "Step 3760: training loss=8.58306884765625E-06\n",
            "Step 3750: training loss=0.00021457672119140625\n",
            "Step 3740: training loss=5.7220458984375E-06\n",
            "Step 3730: training loss=5.7220458984375E-06\n",
            "Step 3720: training loss=4.76837158203125E-06\n",
            "Step 3710: training loss=6.67572021484375E-06\n",
            "Step 3700: training loss=5.7220458984375E-06\n",
            "Step 3690: training loss=3.814697265625E-06\n",
            "Step 3680: training loss=5.7220458984375E-06\n",
            "Step 3670: training loss=3.814697265625E-06\n",
            "Step 3660: training loss=4.76837158203125E-06\n",
            "Step 3650: training loss=0.6170365214347839\n",
            "Step 3640: training loss=6.198883056640625E-05\n",
            "Step 3630: training loss=6.67572021484375E-06\n",
            "Step 3620: training loss=4.76837158203125E-06\n",
            "Step 3610: training loss=9.5367431640625E-06\n",
            "Step 3600: training loss=3.814697265625E-06\n",
            "Step 3590: training loss=6.67572021484375E-06\n",
            "Step 3580: training loss=6.67572021484375E-06\n",
            "Step 3570: training loss=5.7220458984375E-06\n",
            "Step 3560: training loss=7.62939453125E-06\n",
            "Step 3550: training loss=4.76837158203125E-06\n",
            "Step 3540: training loss=4.76837158203125E-06\n",
            "Step 3530: training loss=6.67572021484375E-06\n",
            "Step 3520: training loss=5.7220458984375E-06\n",
            "Step 3510: training loss=6.67572021484375E-06\n",
            "Step 3500: training loss=5.7220458984375E-06\n",
            "Step 3490: training loss=5.7220458984375E-06\n",
            "Step 3480: training loss=3.814697265625E-06\n",
            "Step 3470: training loss=5.7220458984375E-06\n",
            "Step 3460: training loss=4.76837158203125E-06\n",
            "Step 3450: training loss=4.76837158203125E-06\n",
            "Step 3440: training loss=8.58306884765625E-06\n",
            "Step 3430: training loss=0.0001506805419921875\n",
            "Step 3420: training loss=4.76837158203125E-06\n",
            "Step 3410: training loss=4.76837158203125E-06\n",
            "Step 3400: training loss=7.62939453125E-06\n",
            "Step 3390: training loss=4.76837158203125E-06\n",
            "Step 3380: training loss=5.7220458984375E-06\n",
            "Step 3370: training loss=1.430511474609375E-05\n",
            "Step 3360: training loss=6.67572021484375E-06\n",
            "Step 3350: training loss=6.67572021484375E-06\n",
            "Step 3340: training loss=5.7220458984375E-06\n",
            "Step 3330: training loss=5.7220458984375E-06\n",
            "Step 3320: training loss=4.76837158203125E-06\n",
            "Step 3310: training loss=5.7220458984375E-06\n",
            "Step 3300: training loss=1.9073486328125E-05\n",
            "Step 3290: training loss=5.7220458984375E-06\n",
            "Step 3280: training loss=5.7220458984375E-06\n",
            "Step 3270: training loss=9.5367431640625E-06\n",
            "Step 3260: training loss=7.05718994140625E-05\n",
            "Step 3250: training loss=5.7220458984375E-06\n",
            "Step 3240: training loss=1.049041748046875E-05\n",
            "Step 3230: training loss=9.5367431640625E-06\n",
            "Step 3220: training loss=6.67572021484375E-06\n",
            "Step 3210: training loss=4.863739013671875E-05\n",
            "Step 3200: training loss=3.814697265625E-06\n",
            "Step 3190: training loss=1.1444091796875E-05\n",
            "Step 3180: training loss=4.76837158203125E-06\n",
            "Step 3170: training loss=4.76837158203125E-06\n",
            "Step 3160: training loss=3.814697265625E-06\n",
            "Step 3150: training loss=4.76837158203125E-06\n",
            "Step 3140: training loss=4.76837158203125E-06\n",
            "Step 3130: training loss=6.67572021484375E-06\n",
            "Step 3120: training loss=6.67572021484375E-06\n",
            "Step 3110: training loss=1.71661376953125E-05\n",
            "Step 3100: training loss=5.7220458984375E-06\n",
            "Step 3090: training loss=1.9073486328125E-05\n",
            "Step 3080: training loss=6.67572021484375E-06\n",
            "Step 3070: training loss=4.76837158203125E-06\n",
            "Step 3060: training loss=5.7220458984375E-06\n",
            "Step 3050: training loss=6.67572021484375E-06\n",
            "Step 3040: training loss=0.6545015573501587\n",
            "Step 3030: training loss=4.76837158203125E-06\n",
            "Step 3020: training loss=9.5367431640625E-06\n",
            "Step 3010: training loss=6.67572021484375E-06\n",
            "Step 3000: training loss=4.76837158203125E-06\n",
            "Step 2990: training loss=6.67572021484375E-06\n",
            "Step 2980: training loss=5.7220458984375E-06\n",
            "Step 2970: training loss=5.7220458984375E-06\n",
            "Step 2960: training loss=7.62939453125E-06\n",
            "Step 2950: training loss=5.7220458984375E-06\n",
            "Step 2940: training loss=0.06764435768127441\n",
            "Step 2930: training loss=6.67572021484375E-06\n",
            "Step 2920: training loss=6.67572021484375E-06\n",
            "Step 2910: training loss=8.58306884765625E-06\n",
            "Step 2900: training loss=4.76837158203125E-06\n",
            "Step 2890: training loss=1.049041748046875E-05\n",
            "Step 2880: training loss=6.67572021484375E-06\n",
            "Step 2870: training loss=6.67572021484375E-06\n",
            "Step 2860: training loss=6.67572021484375E-06\n",
            "Step 2850: training loss=6.67572021484375E-06\n",
            "Step 2840: training loss=1.1444091796875E-05\n",
            "Step 2830: training loss=0.08737802505493164\n",
            "Step 2820: training loss=6.67572021484375E-06\n",
            "Step 2810: training loss=6.67572021484375E-06\n",
            "Step 2800: training loss=4.76837158203125E-06\n",
            "Step 2790: training loss=5.7220458984375E-06\n",
            "Step 2780: training loss=7.62939453125E-06\n",
            "Step 2770: training loss=4.76837158203125E-06\n",
            "Step 2760: training loss=5.7220458984375E-06\n",
            "Step 2750: training loss=6.67572021484375E-06\n",
            "Step 2740: training loss=7.62939453125E-06\n",
            "Step 2730: training loss=5.7220458984375E-06\n",
            "Step 2720: training loss=0.005813121795654297\n",
            "Step 2710: training loss=5.7220458984375E-06\n",
            "Step 2700: training loss=8.58306884765625E-06\n",
            "Step 2690: training loss=8.58306884765625E-06\n",
            "Step 2680: training loss=5.7220458984375E-06\n",
            "Step 2670: training loss=4.76837158203125E-06\n",
            "Step 2660: training loss=1.049041748046875E-05\n",
            "Step 2650: training loss=5.7220458984375E-06\n",
            "Step 2640: training loss=4.76837158203125E-06\n",
            "Step 2630: training loss=5.7220458984375E-06\n",
            "Step 2620: training loss=6.67572021484375E-06\n",
            "Step 2610: training loss=0.026344776153564453\n",
            "Step 2600: training loss=7.62939453125E-06\n",
            "Step 2590: training loss=5.7220458984375E-06\n",
            "Step 2580: training loss=4.76837158203125E-06\n",
            "Step 2570: training loss=4.76837158203125E-06\n",
            "Step 2560: training loss=7.62939453125E-06\n",
            "Step 2550: training loss=0.0002727508544921875\n",
            "Step 2540: training loss=4.76837158203125E-06\n",
            "Step 2530: training loss=6.67572021484375E-06\n",
            "Step 2520: training loss=7.62939453125E-06\n",
            "Step 2510: training loss=8.58306884765625E-06\n",
            "Step 2500: training loss=5.7220458984375E-06\n",
            "Step 2490: training loss=5.7220458984375E-06\n",
            "Step 2480: training loss=4.76837158203125E-06\n",
            "Step 2470: training loss=3.814697265625E-06\n",
            "Step 2460: training loss=4.76837158203125E-06\n",
            "Step 2450: training loss=4.76837158203125E-06\n",
            "Step 2440: training loss=4.76837158203125E-06\n",
            "Step 2430: training loss=4.76837158203125E-06\n",
            "Step 2420: training loss=4.1961669921875E-05\n",
            "Step 2410: training loss=5.7220458984375E-06\n",
            "Step 2400: training loss=1.239776611328125E-05\n",
            "Step 2390: training loss=6.67572021484375E-06\n",
            "Step 2380: training loss=4.76837158203125E-06\n",
            "Step 2370: training loss=7.62939453125E-06\n",
            "Step 2360: training loss=4.76837158203125E-06\n",
            "Step 2350: training loss=4.76837158203125E-06\n",
            "Step 2340: training loss=5.7220458984375E-06\n",
            "Step 2330: training loss=8.58306884765625E-06\n",
            "Step 2320: training loss=6.67572021484375E-06\n",
            "Step 2310: training loss=7.62939453125E-06\n",
            "Step 2300: training loss=5.7220458984375E-06\n",
            "Step 2290: training loss=6.67572021484375E-06\n",
            "Step 2280: training loss=5.7220458984375E-06\n",
            "Step 2270: training loss=7.62939453125E-06\n",
            "Step 2260: training loss=4.76837158203125E-06\n",
            "Step 2250: training loss=5.7220458984375E-06\n",
            "Step 2240: training loss=9.5367431640625E-06\n",
            "Step 2230: training loss=2.86102294921875E-05\n",
            "Step 2220: training loss=7.62939453125E-06\n",
            "Step 2210: training loss=3.814697265625E-06\n",
            "Step 2200: training loss=7.62939453125E-06\n",
            "Step 2190: training loss=5.7220458984375E-06\n",
            "Step 2180: training loss=0.03591012954711914\n",
            "Step 2170: training loss=1.430511474609375E-05\n",
            "Step 2160: training loss=5.7220458984375E-06\n",
            "Step 2150: training loss=7.62939453125E-06\n",
            "Step 2140: training loss=4.76837158203125E-06\n",
            "Step 2130: training loss=0.13405489921569824\n",
            "Step 2120: training loss=6.67572021484375E-06\n",
            "Step 2110: training loss=6.67572021484375E-06\n",
            "Step 2100: training loss=3.0517578125E-05\n",
            "Step 2090: training loss=8.58306884765625E-06\n",
            "Step 2080: training loss=1.430511474609375E-05\n",
            "Step 2070: training loss=6.67572021484375E-06\n",
            "Step 2060: training loss=6.67572021484375E-06\n",
            "Step 2050: training loss=5.7220458984375E-06\n",
            "Step 2040: training loss=2.47955322265625E-05\n",
            "Step 2030: training loss=7.62939453125E-06\n",
            "Step 2020: training loss=5.7220458984375E-06\n",
            "Step 2010: training loss=6.67572021484375E-06\n",
            "Step 2000: training loss=7.62939453125E-06\n",
            "Step 1990: training loss=5.435943603515625E-05\n",
            "Step 1980: training loss=0.002093791961669922\n",
            "Step 1970: training loss=6.67572021484375E-06\n",
            "Step 1960: training loss=6.67572021484375E-06\n",
            "Step 1950: training loss=1.049041748046875E-05\n",
            "Step 1940: training loss=7.62939453125E-06\n",
            "Step 1930: training loss=7.62939453125E-06\n",
            "Step 1920: training loss=7.62939453125E-06\n",
            "Step 1910: training loss=4.76837158203125E-06\n",
            "Step 1900: training loss=6.67572021484375E-06\n",
            "Step 1890: training loss=5.7220458984375E-06\n",
            "Step 1880: training loss=1.1444091796875E-05\n",
            "Step 1870: training loss=4.76837158203125E-06\n",
            "Step 1860: training loss=9.5367431640625E-06\n",
            "Step 1850: training loss=5.7220458984375E-06\n",
            "Step 1840: training loss=6.67572021484375E-06\n",
            "Step 1830: training loss=5.7220458984375E-06\n",
            "Step 1820: training loss=7.62939453125E-06\n",
            "Step 1810: training loss=7.62939453125E-06\n",
            "Step 1800: training loss=5.7220458984375E-06\n",
            "Step 1790: training loss=9.5367431640625E-06\n",
            "Step 1780: training loss=7.62939453125E-06\n",
            "Step 1770: training loss=7.62939453125E-06\n",
            "Step 1760: training loss=7.62939453125E-06\n",
            "Step 1750: training loss=9.5367431640625E-06\n",
            "Step 1740: training loss=6.67572021484375E-06\n",
            "Step 1730: training loss=7.62939453125E-06\n",
            "Step 1720: training loss=6.67572021484375E-06\n",
            "Step 1710: training loss=7.62939453125E-06\n",
            "Step 1700: training loss=0.6357942819595337\n",
            "Step 1690: training loss=5.7220458984375E-06\n",
            "Step 1680: training loss=2.288818359375E-05\n",
            "Step 1670: training loss=5.7220458984375E-06\n",
            "Step 1660: training loss=4.57763671875E-05\n",
            "Step 1650: training loss=1.049041748046875E-05\n",
            "Step 1640: training loss=6.67572021484375E-06\n",
            "Step 1630: training loss=9.5367431640625E-06\n",
            "Step 1620: training loss=8.58306884765625E-06\n",
            "Step 1610: training loss=9.5367431640625E-06\n",
            "Step 1600: training loss=0.0006923675537109375\n",
            "Step 1590: training loss=3.814697265625E-06\n",
            "Step 1580: training loss=0.691277801990509\n",
            "Step 1570: training loss=9.5367431640625E-06\n",
            "Step 1560: training loss=1.1444091796875E-05\n",
            "Step 1550: training loss=1.239776611328125E-05\n",
            "Step 1540: training loss=0.03815627098083496\n",
            "Step 1530: training loss=1.049041748046875E-05\n",
            "Step 1520: training loss=1.811981201171875E-05\n",
            "Step 1510: training loss=7.62939453125E-06\n",
            "Step 1500: training loss=7.62939453125E-06\n",
            "Step 1490: training loss=9.632110595703125E-05\n",
            "Step 1480: training loss=7.62939453125E-06\n",
            "Step 1470: training loss=7.62939453125E-06\n",
            "Step 1460: training loss=7.62939453125E-06\n",
            "Step 1450: training loss=6.67572021484375E-06\n",
            "Step 1440: training loss=1.1444091796875E-05\n",
            "Step 1430: training loss=9.5367431640625E-06\n",
            "Step 1420: training loss=8.58306884765625E-06\n",
            "Step 1410: training loss=2.384185791015625E-05\n",
            "Step 1400: training loss=8.58306884765625E-06\n",
            "Step 1390: training loss=7.62939453125E-06\n",
            "Step 1380: training loss=9.5367431640625E-06\n",
            "Step 1370: training loss=6.67572021484375E-06\n",
            "Step 1360: training loss=0.610305905342102\n",
            "Step 1350: training loss=7.62939453125E-06\n",
            "Step 1340: training loss=1.049041748046875E-05\n",
            "Step 1330: training loss=1.1444091796875E-05\n",
            "Step 1320: training loss=0.6226256489753723\n",
            "Step 1310: training loss=9.5367431640625E-06\n",
            "Step 1300: training loss=7.62939453125E-06\n",
            "Step 1290: training loss=9.5367431640625E-06\n",
            "Step 1280: training loss=1.430511474609375E-05\n",
            "Step 1270: training loss=1.430511474609375E-05\n",
            "Step 1260: training loss=8.58306884765625E-06\n",
            "Step 1250: training loss=1.049041748046875E-05\n",
            "Step 1240: training loss=1.049041748046875E-05\n",
            "Step 1230: training loss=1.049041748046875E-05\n",
            "Step 1220: training loss=1.049041748046875E-05\n",
            "Step 1210: training loss=8.58306884765625E-06\n",
            "Step 1200: training loss=3.24249267578125E-05\n",
            "Step 1190: training loss=0.0032091140747070312\n",
            "Step 1180: training loss=1.239776611328125E-05\n",
            "Step 1170: training loss=6.771087646484375E-05\n",
            "Step 1160: training loss=7.62939453125E-06\n",
            "Step 1150: training loss=1.1444091796875E-05\n",
            "Step 1140: training loss=1.239776611328125E-05\n",
            "Step 1130: training loss=3.147125244140625E-05\n",
            "Step 1120: training loss=0.6270384788513184\n",
            "Step 1110: training loss=1.33514404296875E-05\n",
            "Step 1100: training loss=1.1444091796875E-05\n",
            "Step 1090: training loss=1.33514404296875E-05\n",
            "Step 1080: training loss=1.1444091796875E-05\n",
            "Step 1070: training loss=1.52587890625E-05\n",
            "Step 1060: training loss=2.002716064453125E-05\n",
            "Step 1050: training loss=1.049041748046875E-05\n",
            "Step 1040: training loss=1.239776611328125E-05\n",
            "Step 1030: training loss=1.33514404296875E-05\n",
            "Step 1020: training loss=0.000179290771484375\n",
            "Step 1010: training loss=0.0012784004211425781\n",
            "Step 1000: training loss=3.528594970703125E-05\n",
            "Step 990: training loss=1.239776611328125E-05\n",
            "Step 980: training loss=1.811981201171875E-05\n",
            "Step 970: training loss=0.0006151199340820312\n",
            "Step 960: training loss=2.002716064453125E-05\n",
            "Step 950: training loss=1.811981201171875E-05\n",
            "Step 940: training loss=1.621246337890625E-05\n",
            "Step 930: training loss=2.6702880859375E-05\n",
            "Step 920: training loss=3.62396240234375E-05\n",
            "Step 910: training loss=1.9073486328125E-05\n",
            "Step 900: training loss=2.002716064453125E-05\n",
            "Step 890: training loss=1.9073486328125E-05\n",
            "Step 880: training loss=0.0624086856842041\n",
            "Step 870: training loss=0.00138092041015625\n",
            "Step 860: training loss=0.0013418197631835938\n",
            "Step 850: training loss=5.245208740234375E-05\n",
            "Step 840: training loss=2.002716064453125E-05\n",
            "Step 830: training loss=2.574920654296875E-05\n",
            "Step 820: training loss=2.574920654296875E-05\n",
            "Step 810: training loss=0.012196063995361328\n",
            "Step 800: training loss=0.00176239013671875\n",
            "Step 790: training loss=0.5621439218521118\n",
            "Step 780: training loss=2.765655517578125E-05\n",
            "Step 770: training loss=0.0008392333984375\n",
            "Step 760: training loss=4.9591064453125E-05\n",
            "Step 750: training loss=4.57763671875E-05\n",
            "Step 740: training loss=7.82012939453125E-05\n",
            "Step 730: training loss=4.291534423828125E-05\n",
            "Step 720: training loss=0.0817406177520752\n",
            "Step 710: training loss=4.1961669921875E-05\n",
            "Step 700: training loss=4.38690185546875E-05\n",
            "Step 690: training loss=0.00045108795166015625\n",
            "Step 680: training loss=0.01776123046875\n",
            "Step 670: training loss=0.005181789398193359\n",
            "Step 660: training loss=7.534027099609375E-05\n",
            "Step 650: training loss=0.0002307891845703125\n",
            "Step 640: training loss=0.07984590530395508\n",
            "Step 630: training loss=0.000148773193359375\n",
            "Step 620: training loss=0.00025844573974609375\n",
            "Step 610: training loss=0.0004811286926269531\n",
            "Step 600: training loss=0.00042724609375\n",
            "Step 590: training loss=0.0008397102355957031\n",
            "Step 580: training loss=0.019790172576904297\n",
            "Step 570: training loss=0.012163639068603516\n",
            "Step 560: training loss=0.0026197433471679688\n",
            "Step 550: training loss=0.10431981086730957\n",
            "Step 540: training loss=0.18549251556396484\n",
            "Step 530: training loss=0.17843544483184814\n",
            "Step 520: training loss=0.09172487258911133\n",
            "Step 510: training loss=0.2530078887939453\n",
            "Step 500: training loss=0.2742502689361572\n",
            "Step 490: training loss=0.45098185539245605\n",
            "Step 480: training loss=0.7335887551307678\n",
            "Step 470: training loss=0.3340989351272583\n",
            "Step 460: training loss=0.46728062629699707\n",
            "Step 450: training loss=0.43637311458587646\n",
            "Step 440: training loss=0.3932632803916931\n",
            "Step 430: training loss=0.5657820105552673\n",
            "Step 420: training loss=0.6075621247291565\n",
            "Step 410: training loss=0.34535038471221924\n",
            "Step 400: training loss=0.60452800989151\n",
            "Step 390: training loss=0.5014255046844482\n",
            "Step 380: training loss=0.6174139976501465\n",
            "Step 370: training loss=0.540941059589386\n",
            "Step 360: training loss=0.5960986614227295\n",
            "Step 350: training loss=0.5904781818389893\n",
            "Step 340: training loss=0.6335801482200623\n",
            "Step 330: training loss=0.6405213475227356\n",
            "Step 320: training loss=0.6378403902053833\n",
            "Step 310: training loss=0.5431717038154602\n",
            "Step 300: training loss=0.6779217720031738\n",
            "Step 290: training loss=0.876424252986908\n",
            "Step 280: training loss=0.6475299596786499\n",
            "Step 270: training loss=0.6308478713035583\n",
            "Step 260: training loss=0.6912044286727905\n",
            "Step 250: training loss=0.6098794341087341\n",
            "Step 240: training loss=0.6823691725730896\n",
            "Step 230: training loss=0.6101388335227966\n",
            "Step 220: training loss=0.6501943469047546\n",
            "Step 210: training loss=0.6916876435279846\n",
            "Step 200: training loss=1.279041051864624\n",
            "Step 190: training loss=0.6148421168327332\n",
            "Step 180: training loss=0.6868743896484375\n",
            "Step 170: training loss=0.7629075050354004\n",
            "Step 160: training loss=0.6814028024673462\n",
            "Step 150: training loss=0.6624190807342529\n",
            "Step 140: training loss=0.6596514582633972\n",
            "Step 130: training loss=0.6654109954833984\n",
            "Step 120: training loss=0.6651424169540405\n",
            "Step 110: training loss=0.6856555938720703\n",
            "Step 100: training loss=0.7268278002738953\n",
            "Step 90: training loss=0.7079674601554871\n",
            "Step 80: training loss=0.6738218665122986\n",
            "Step 70: training loss=0.7437894344329834\n",
            "Step 60: training loss=1.1394693851470947\n",
            "Step 50: training loss=0.6954395771026611\n",
            "Step 40: training loss=0.6955251693725586\n",
            "Step 30: training loss=0.706077516078949\n",
            "Step 20: training loss=0.7153997421264648\n",
            "Step 10: training loss=0.7283068299293518\n",
            "Step 1: training loss=0.6786144375801086\n",
            "Created results file: file-697f5d7ff2b14ced8258a9c152973400\n",
            "Training started.\n",
            "Job queued, waiting for GPUs.\n",
            "Finetuning started.\n",
            "Data Import started.\n",
            "Job started.\n",
            "Preprocessing completed for file validation file.\n",
            "Preprocessing completed for file training file.\n",
            "Preprocessing running for file training file.\n",
            "Job enqueued. Waiting for jobs ahead to complete.\n"
          ]
        }
      ],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Fine-tuned Model ID: gpt-4.1-mini-2025-04-14.ft-4cad7de198a34baeb4f0c95ff01ac844\n"
          ]
        }
      ],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploying fine-tuned model: gpt-4.1-mini-2025-04-14.ft-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Waiting for deployment to complete...\n",
            "Model deployment completed: gpt-4.1-mini-dpo-finetuned\n"
          ]
        }
      ],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4.1-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=200)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    print(\"Waiting for deployment to complete...\")\n",
        "    deployment.result()\n",
        "\n",
        "print(f\"Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing fine-tuned model via deployment: gpt-4.1-mini-dpo-finetuned\n",
            "Model response: Machine learning is like teaching a computer to learn from experience, similar to how people do. Instead of programming specific instructions for every task, we give the computer a lot of data and it figures out patterns on its own. Then, it can use what it learned to make decisions or predictions. For example, if you show a machine learning system lots of pictures of cats and dogs, it will learn to recognize which is which by itself.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate Fine-Tuned Model\n",
        "\n",
        "Evaluate your model using Azure AI Evaluation SDK to measure quality improvements from DPO fine-tuning.\n",
        "\n",
        "We'll assess 3 key metrics:\n",
        "- **Coherence**: Logical flow and structure\n",
        "- **Fluency**: Grammatical correctness and naturalness\n",
        "- **Groundedness**: Factual accuracy against context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating fine-tuned model: gpt-4.1-mini-dpo-finetuned\n",
            "Using base model as evaluator: gpt-4.1-mini\n",
            "\n",
            "Evaluating deployment: gpt-4.1-mini-dpo-finetuned\n",
            "Using evaluator model: gpt-4.1-mini\n",
            "Using 50 samples from training.jsonl\n",
            "Generating model responses...\n",
            "  Processed 10/50\n",
            "  Processed 20/50\n",
            "  Processed 30/50\n",
            "  Processed 40/50\n",
            "  Processed 50/50\n",
            "Running evaluation with 3 metrics...\n",
            "2026-01-05 14:50:06 +0530   24016 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:06 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 8.14 seconds. Estimated time for incomplete lines: 398.86 seconds.\n",
            "2026-01-05 14:50:07 +0530   24016 execution.bulk     INFO     Finished 2 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 4.35 seconds. Estimated time for incomplete lines: 208.8 seconds.\n",
            "2026-01-05 14:50:07 +0530   22800 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 8.87 seconds. Estimated time for incomplete lines: 434.63 seconds.\n",
            "2026-01-05 14:50:07 +0530   28448 execution.bulk     INFO     Finished 1 / 50 lines.\n",
            "2026-01-05 14:50:07 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 9.01 seconds. Estimated time for incomplete lines: 441.49 seconds.\n",
            "2026-01-05 14:50:09 +0530   28448 execution.bulk     INFO     Finished 4 / 50 lines.\n",
            "2026-01-05 14:50:09 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 2.73 seconds. Estimated time for incomplete lines: 125.58 seconds.\n",
            "2026-01-05 14:50:09 +0530   24016 execution.bulk     INFO     Finished 6 / 50 lines.\n",
            "2026-01-05 14:50:09 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.83 seconds. Estimated time for incomplete lines: 80.52 seconds.\n",
            "2026-01-05 14:50:10 +0530   22800 execution.bulk     INFO     Finished 7 / 50 lines.\n",
            "2026-01-05 14:50:10 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.76 seconds. Estimated time for incomplete lines: 75.68 seconds.\n",
            "2026-01-05 14:50:11 +0530   24016 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:11 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.32 seconds. Estimated time for incomplete lines: 52.8 seconds.\n",
            "2026-01-05 14:50:12 +0530   22800 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:12 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.39 seconds. Estimated time for incomplete lines: 55.6 seconds.\n",
            "2026-01-05 14:50:12 +0530   28448 execution.bulk     INFO     Finished 10 / 50 lines.\n",
            "2026-01-05 14:50:12 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.44 seconds. Estimated time for incomplete lines: 57.6 seconds.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.43 seconds. Estimated time for incomplete lines: 55.77 seconds.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Finished 12 / 50 lines.\n",
            "2026-01-05 14:50:14 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.36 seconds. Estimated time for incomplete lines: 51.68 seconds.\n",
            "2026-01-05 14:50:15 +0530   22800 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:15 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 59.67 seconds.\n",
            "2026-01-05 14:50:15 +0530   28448 execution.bulk     INFO     Finished 11 / 50 lines.\n",
            "2026-01-05 14:50:15 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.6 seconds. Estimated time for incomplete lines: 62.4 seconds.\n",
            "2026-01-05 14:50:16 +0530   22800 execution.bulk     INFO     Finished 14 / 50 lines.\n",
            "2026-01-05 14:50:16 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.32 seconds. Estimated time for incomplete lines: 47.52 seconds.\n",
            "2026-01-05 14:50:17 +0530   24016 execution.bulk     INFO     Finished 17 / 50 lines.\n",
            "2026-01-05 14:50:17 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.12 seconds. Estimated time for incomplete lines: 36.96 seconds.\n",
            "2026-01-05 14:50:17 +0530   28448 execution.bulk     INFO     Finished 14 / 50 lines.\n",
            "2026-01-05 14:50:17 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.4 seconds. Estimated time for incomplete lines: 50.4 seconds.\n",
            "2026-01-05 14:50:18 +0530   24016 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:18 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 30.9 seconds.\n",
            "2026-01-05 14:50:20 +0530   22800 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:20 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 32.7 seconds.\n",
            "2026-01-05 14:50:21 +0530   28448 execution.bulk     INFO     Finished 20 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.15 seconds. Estimated time for incomplete lines: 34.5 seconds.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 31.9 seconds.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Finished 22 / 50 lines.\n",
            "2026-01-05 14:50:21 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 29.96 seconds.\n",
            "2026-01-05 14:50:22 +0530   24016 execution.bulk     INFO     Finished 24 / 50 lines.\n",
            "2026-01-05 14:50:22 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 1.02 seconds. Estimated time for incomplete lines: 26.52 seconds.\n",
            "2026-01-05 14:50:23 +0530   22800 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:23 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.19 seconds. Estimated time for incomplete lines: 34.51 seconds.\n",
            "2026-01-05 14:50:24 +0530   28448 execution.bulk     INFO     Finished 21 / 50 lines.\n",
            "2026-01-05 14:50:24 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.23 seconds. Estimated time for incomplete lines: 35.67 seconds.\n",
            "2026-01-05 14:50:25 +0530   28448 execution.bulk     INFO     Finished 23 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 31.59 seconds.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Finished 29 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 19.53 seconds.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:25 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 18.4 seconds.\n",
            "2026-01-05 14:50:26 +0530   28448 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:26 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 18.4 seconds.\n",
            "2026-01-05 14:50:26 +0530   22800 execution.bulk     INFO     Finished 26 / 50 lines.\n",
            "2026-01-05 14:50:26 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 25.68 seconds.\n",
            "2026-01-05 14:50:27 +0530   24016 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:27 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 18.05 seconds.\n",
            "2026-01-05 14:50:28 +0530   22800 execution.bulk     INFO     Finished 30 / 50 lines.\n",
            "2026-01-05 14:50:28 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 20.0 seconds.\n",
            "2026-01-05 14:50:28 +0530   24016 execution.bulk     INFO     Finished 32 / 50 lines.\n",
            "2026-01-05 14:50:28 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 17.1 seconds.\n",
            "2026-01-05 14:50:29 +0530   24016 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 14:50:29 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 15.98 seconds.\n",
            "2026-01-05 14:50:30 +0530   24016 execution.bulk     INFO     Finished 36 / 50 lines.\n",
            "2026-01-05 14:50:30 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 12.46 seconds.\n",
            "2026-01-05 14:50:31 +0530   22800 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:31 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 20.33 seconds.\n",
            "2026-01-05 14:50:32 +0530   28448 execution.bulk     INFO     Finished 31 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 20.9 seconds.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 8.6 seconds.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:32 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 7.47 seconds.\n",
            "2026-01-05 14:50:33 +0530   28448 execution.bulk     INFO     Finished 33 / 50 lines.\n",
            "2026-01-05 14:50:33 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 18.19 seconds.\n",
            "2026-01-05 14:50:34 +0530   22800 execution.bulk     INFO     Finished 37 / 50 lines.\n",
            "2026-01-05 14:50:34 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 12.74 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 7.04 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 6.09 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 5.1 seconds.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:35 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 4.15 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.82 seconds. Estimated time for incomplete lines: 3.28 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.8 seconds. Estimated time for incomplete lines: 2.4 seconds.\n",
            "2026-01-05 14:50:36 +0530   22800 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 9.5 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.79 seconds. Estimated time for incomplete lines: 1.58 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.78 seconds. Estimated time for incomplete lines: 0.78 seconds.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:36 +0530   24016 execution.bulk     INFO     Average execution time for completed lines: 0.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Finished 39 / 50 lines.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 10.89 seconds.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Finished 40 / 50 lines.\n",
            "2026-01-05 14:50:37 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 9.8 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"coherence_20260105_091958_326856\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.326856+00:00\"\n",
            "Duration: \"0:00:39.278192\"\n",
            "\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 9.0 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 7.84 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 6.65 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 5.58 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 4.55 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 3.56 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 2.64 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 1.72 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.84 seconds. Estimated time for incomplete lines: 0.84 seconds.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:39 +0530   22800 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"fluency_20260105_091958_332852\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.332852+00:00\"\n",
            "Duration: \"0:00:42.034842\"\n",
            "\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 41 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 9.27 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 42 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 8.0 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 43 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 6.86 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 44 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 5.76 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 45 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 4.7 seconds.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Finished 46 / 50 lines.\n",
            "2026-01-05 14:50:40 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 3.68 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 47 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 2.73 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 48 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 1.8 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 49 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 0.88 seconds.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Finished 50 / 50 lines.\n",
            "2026-01-05 14:50:41 +0530   28448 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"groundedness_20260105_091958_336895\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-05 09:19:58.336895+00:00\"\n",
            "Duration: \"0:00:43.737933\"\n",
            "\n",
            "======= Combined Run Summary (Per Evaluator) =======\n",
            "\n",
            "{\n",
            "    \"coherence\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:39.278192\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"fluency\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:42.034842\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"groundedness\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:43.737933\",\n",
            "        \"completed_lines\": 50,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    }\n",
            "}\n",
            "\n",
            "====================================================\n",
            "\n",
            "Evaluation results saved to \"C:\\work\\AMLRepos\\fine-tuning\\Demos\\DPO_Intel_Orca\\evaluation_results_gpt_4.1_mini_dpo_finetuned\".\n",
            "\n",
            "EVALUATION RESULTS: gpt-4.1-mini-dpo-finetuned\n",
            "\n",
            "Coherence:      3.8400 (1-5 scale)\n",
            "Fluency:        2.7000 (1-5 scale)\n",
            "Groundedness:   3.1000 (1-5 scale)\n",
            "============================================================\n",
            "Detailed results: ./evaluation_results_gpt_4.1_mini_dpo_finetuned\n",
            "\n",
            "Compare base model vs fine-tuned model metrics to see DPO improvements!\n"
          ]
        }
      ],
      "source": [
        "base_model = os.getenv(\"DEPLOYMENT_NAME\")  # Use base model for evaluation\n",
        "\n",
        "print(f\"Evaluating fine-tuned model: {deployment_name}\")\n",
        "\n",
        "finetuned_results = evaluate_model(deployment_name, num_samples=50, evaluator_model=base_model)\n",
        "\n",
        "print(\"\\nCompare base model vs fine-tuned model metrics to see DPO improvements!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.1 Model Comparison Results\n",
        "\n",
        "Below is an example comparison between base model and fine-tuned model evaluation results (using 50 samples):\n",
        "\n",
        "| Metric | Base Model | Fine-Tuned Model | Change | Status |\n",
        "|--------|-----------|------------------|--------|--------|\n",
        "| **Coherence** | 4.3000 | 3.8400 | -0.4600 | Decreased |\n",
        "| **Fluency** | 3.5800 | 2.7000 | -0.8800 | Decreased |\n",
        "| **Groundedness** | 4.1000 | 3.1000 | -1.0000 | Decreased |\n",
        "\n",
        "### Understanding the Results\n",
        "\n",
        "The example above shows that the fine-tuned model performed worse than the base model across all metrics. This indicates that the DPO training did not improve model quality with the current configuration.\n",
        "\n",
        "### How to Improve Results\n",
        "\n",
        "To achieve better fine-tuned model performance, experiment with:\n",
        "\n",
        "**1. Hyperparameter Tuning:**\n",
        "- **Reduce epochs**: Try `n_epochs=1` or `n_epochs=2` to prevent overfitting\n",
        "- **Lower learning rate**: Set `learning_rate_multiplier=0.5` or `0.1` for more conservative training\n",
        "- **Adjust batch size**: Keep at 1-2 for DPO stability\n",
        "\n",
        "**2. Training Data:**\n",
        "- **Increase sample size**: Use more training examples (e.g., 100-1000 samples)\n",
        "- **Verify data quality**: Ensure \"preferred_output\" responses are truly higher quality than rejected ones\n",
        "- **Review data format**: Confirm DPO pairs are correctly labeled\n",
        "\n",
        "**3. Evaluation Settings:**\n",
        "- **Increase evaluation samples**: Use `num_samples=100` or more for more reliable metrics\n",
        "- **Test on different data**: Evaluate on a separate validation set, not training data\n",
        "\n",
        "### Success Indicators\n",
        "\n",
        "Your fine-tuning is successful when you see **positive changes** like:\n",
        "- Coherence: +0.5 or higher\n",
        "- Fluency: +0.3 or higher  \n",
        "- Groundedness: +0.4 or higher\n",
        "\n",
        "Iterate on hyperparameters and training data until your fine-tuned model consistently outperforms the base model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Continual Fine-Tuning (Optional)\n",
        "\n",
        "If your fine-tuned model didn't show improvements, you can perform **continual fine-tuning** by using the fine-tuned model as the base for another round of training. This iterative approach can help refine the model further.\n",
        "\n",
        "### When to Use Continual Fine-Tuning:\n",
        "- Your first fine-tuning run didn't improve metrics\n",
        "- You want to adjust hyperparameters and train further\n",
        "- You have additional training data to incorporate\n",
        "- You need to fine-tune for a more specific task\n",
        "\n",
        "### How It Works:\n",
        "Instead of using model_name (base model), use fine_tuned_model_id from section 10 as your new base model. The code below is the same as section 8, but modified to continue training from your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Continual fine-tuning using base model: gpt-4.1-mini-2025-04-14.ft-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Continual fine-tuning job created!\n",
            "Job ID: ftjob-ca7805d53a5e496ca87aebca9894e134\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "\n",
        "continual_base_model = fine_tuned_model_id\n",
        "print(f\"Continual fine-tuning using base model: {continual_base_model}\")\n",
        "\n",
        "continual_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,  # You can use the same or upload new training data\n",
        "    validation_file=validation_file.id,\n",
        "    model=continual_base_model,  # Using fine-tuned model instead of base model\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 2,  # Reduced from 3 to prevent overfitting\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 0.5  # Lower learning rate for fine-tuning refinement\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\"Continual fine-tuning job created!\")\n",
        "print(f\"Job ID: {continual_job.id}\")\n",
        "print(f\"Status: {continual_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "envrft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
